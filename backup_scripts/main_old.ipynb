{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:50:28.772900Z",
     "iopub.status.busy": "2023-11-13T16:50:28.772514Z",
     "iopub.status.idle": "2023-11-13T16:50:43.604811Z",
     "shell.execute_reply": "2023-11-13T16:50:43.603822Z",
     "shell.execute_reply.started": "2023-11-13T16:50:28.772868Z"
    }
   },
   "source": [
    "%pip install numpy==1.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T17:51:59.678557Z",
     "iopub.status.busy": "2023-11-13T17:51:59.678200Z",
     "iopub.status.idle": "2023-11-13T17:51:59.688582Z",
     "shell.execute_reply": "2023-11-13T17:51:59.687452Z",
     "shell.execute_reply.started": "2023-11-13T17:51:59.678528Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m \u001b[39mimport\u001b[39;00m io, transform\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "\n",
    "debug = logging.getLogger(\"Debug\")\n",
    "info  = print\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:50:28.772900Z",
     "iopub.status.busy": "2023-11-13T16:50:28.772514Z",
     "iopub.status.idle": "2023-11-13T16:50:43.604811Z",
     "shell.execute_reply": "2023-11-13T16:50:43.603822Z",
     "shell.execute_reply.started": "2023-11-13T16:50:28.772868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MPS Mode: mps\n"
     ]
    }
   ],
   "source": [
    "#check GPU\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running CUDA Mode:\", device, torch.cuda.get_device_name(0))\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Running MPS Mode:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running CPU Mode:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Classes\n",
    "- Create Dataloader class\n",
    "\n",
    "Note: Working on Part (a) as of now.  \n",
    "Guiding light: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T17:34:51.523866Z",
     "iopub.status.busy": "2023-11-13T17:34:51.523123Z",
     "iopub.status.idle": "2023-11-13T17:34:51.532580Z",
     "shell.execute_reply": "2023-11-13T17:34:51.531555Z",
     "shell.execute_reply.started": "2023-11-13T17:34:51.523832Z"
    }
   },
   "outputs": [],
   "source": [
    "START_TOKEN = \"START\"\n",
    "END_TOKEN = \"END\"\n",
    "UNK_TOKEN = \"UNK\"\n",
    "\n",
    "# MAX_EXAMPLES = 100\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_dict, wd_to_id, id_to_wd):\n",
    "        self.freq_dict = freq_dict\n",
    "        self.wd_to_id = wd_to_id\n",
    "        self.id_to_wd = id_to_wd\n",
    "        self.N = len(freq_dict)\n",
    "    \n",
    "    def get_id(self, word):\n",
    "        if word in self.wd_to_id:\n",
    "            return self.wd_to_id[word]\n",
    "        else:\n",
    "            return self.wd_to_id[UNK_TOKEN]\n",
    "\n",
    "class LatexFormulaDataset(Dataset):\n",
    "    \"\"\"Latex Formula Dataset: Image and Text\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, max_examples=None, transform = None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with image name and text\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        #@TODO: May want to preload images\n",
    "        info(\"Loading Dataset...\")\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        info(\"Loaded Dataset\", self.df.info)\n",
    "        \n",
    "        #Slice the dataset if max_examples is not None\n",
    "        if max_examples is not None:\n",
    "            self.df = self.df.iloc[:max_examples, :]\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.vocab= self.construct_vocab()  \n",
    "\n",
    "        self.maxlen = 0\n",
    "        for formula in self.df['formula']:\n",
    "            if len(formula) > self.maxlen:\n",
    "                self.maxlen = len(formula)\n",
    "\n",
    "        self.df['formula'] = self.df['formula'].apply(lambda x: [START_TOKEN] + x + [END_TOKEN] + [UNK_TOKEN]*(self.maxlen - len(x)))\n",
    "        self.maxlen += 2 #for start and end tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns sample of type image, textformula\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.df.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        formula = self.df.iloc[idx, 1]\n",
    "        formula = np.array([formula], dtype=str).reshape(-1, 1)\n",
    "        formula = [[self.vocab.get_id(wd[0]) for wd in formula]] \n",
    "        sample = {'image': image, 'formula': torch.tensor(formula, dtype=torch.int64)}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            \n",
    "        return sample \n",
    "    \n",
    "    def construct_vocab(self):\n",
    "        \"\"\"\n",
    "        Constructs vocabulary from the dataset formulas\n",
    "        \"\"\"\n",
    "        #Split on spaces to tokenize\n",
    "        self.df['formula'] = self.df['formula'].apply(lambda x: x.split())\n",
    "\n",
    "        freq_dict = {}\n",
    "        for formula in self.df['formula']:\n",
    "            for wd in formula:\n",
    "                if wd not in freq_dict:\n",
    "                    freq_dict[wd] = 1\n",
    "                else:\n",
    "                    freq_dict[wd] += 1\n",
    "        freq_dict[START_TOKEN] = 1\n",
    "        freq_dict[END_TOKEN] = 1\n",
    "        freq_dict[UNK_TOKEN] = 1\n",
    "        N = len(freq_dict)\n",
    "        wd_to_id = {}\n",
    "        for i, wd in enumerate(freq_dict):\n",
    "            wd_to_id[wd] = i\n",
    "        id_to_wd = {v: k for k, v in wd_to_id.items()}\n",
    "    \n",
    "        #pad the formulas with \n",
    "        return Vocabulary(freq_dict, wd_to_id, id_to_wd)      \n",
    "\n",
    "def get_dataloader(csv_path, image_root, batch_size, transform = None, max_examples = None):\n",
    "    \"\"\"\n",
    "    Returns dataloader,dataset for the dataset\n",
    "    \"\"\"\n",
    "    dataset = LatexFormulaDataset(csv_path, image_root, max_examples=max_examples,transform=transform) #checked\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader, dataset\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Network\n",
    "- A CNN to encode image to more meaningful vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T18:15:50.390411Z",
     "iopub.status.busy": "2023-11-13T18:15:50.390048Z",
     "iopub.status.idle": "2023-11-13T18:15:50.401998Z",
     "shell.execute_reply": "2023-11-13T18:15:50.401089Z",
     "shell.execute_reply.started": "2023-11-13T18:15:50.390380Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        #@TODO:reduce number of layers: eliminate pools and acts\n",
    "        self.conv1 = nn.Conv2d(3, 32, (5, 5))\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, (5, 5))\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, (5, 5))\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, (5, 5))\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, (5, 5))\n",
    "        self.act5 = nn.ReLU()\n",
    "        self.pool5 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.avg_pool = nn.AvgPool2d((3, 3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.act2(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.act3(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.act4(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = self.act5(self.conv5(x))\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(-1,512) \n",
    "        # info(f\"Encoder Output Shape: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T18:15:56.106481Z",
     "iopub.status.busy": "2023-11-13T18:15:56.105734Z",
     "iopub.status.idle": "2023-11-13T18:15:56.198416Z",
     "shell.execute_reply": "2023-11-13T18:15:56.197693Z",
     "shell.execute_reply.started": "2023-11-13T18:15:56.106446Z"
    }
   },
   "source": [
    "### Vocabulary\n",
    "- https://github.com/harvardnlp/im2markup/blob/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    (here M is whatever the batch size is passed)\n",
    "\n",
    "    context_size : size of the context vector [shape: (1,M,context_size)]\n",
    "    n_layers: number of layers [for our purposes, defaults to 1]\n",
    "    hidden_size : size of the hidden state vectors [shape: (n_layers,M,hidden_size)]\n",
    "    embed_size : size of the embedding vectors [shape: (1,M,embed_size)]\n",
    "    vocab_size : size of the vocabulary\n",
    "    max_length : maximum length of the formula\n",
    "    \"\"\"\n",
    "    def __init__(self, context_size, vocab, n_layers = 1, hidden_size = 512, embed_size = 512,  max_length = 100):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab.N\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "\n",
    "        self.input_size = context_size + embed_size\n",
    "        self.embed = nn.Embedding(self.vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, self.vocab_size)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "    \n",
    "    def forward(self, context, target_tensor = None):\n",
    "        \"\"\"\n",
    "        M: batch_size\n",
    "        context is the context vector from the encoder [shape: (M,context_size)]\n",
    "        target_tensor is the formula in tensor form [shape: (M,max_length)] (in the second dimension, it is sequence of indices of formula tokens)\n",
    "            if target_tensor is not None, then we are in Teacher Forcing mode\n",
    "            else normal jo bhi (last prediction ka embed is concatenated)\n",
    "        \"\"\"\n",
    "        # info(\"Decoder Forward\")\n",
    "        # info(f\"Context shape: {context.shape}\")\n",
    "        context.to(device)\n",
    "        batch_size = context.shape[0]\n",
    "\n",
    "        #initialize hidden state and cell state\n",
    "            #@TODO: Some caveat in the size of the cell state. Should it be same as hidden_size? (check nn.LSTM documentation)\n",
    "        hidden = torch.zeros((batch_size, self.hidden_size)).to(context.device)\n",
    "        cell = torch.zeros((batch_size, self.hidden_size)).to(context.device)\n",
    "\n",
    "        #initialize the input with embedding of the start token\n",
    "        init_embed = self.embed(torch.tensor([self.vocab.wd_to_id[START_TOKEN]]).to(device)).reshape((1, self.embed_size))\n",
    "        init_embed = torch.repeat_interleave(init_embed, batch_size, dim = 0).to(context.device)\n",
    "\n",
    "        # info(f\"Initial Embedding Shape: {init_embed.shape}\")\n",
    "\n",
    "        input = torch.cat([context, init_embed], dim = 1).to(context.device)\n",
    "\n",
    "        #initialize the output_history and init_output\n",
    "        outputs = []\n",
    "        output = torch.zeros((batch_size, self.vocab_size)).to(context.device)\n",
    "        \n",
    "        \n",
    "        for i in range(self.max_length):\n",
    "            hidden, cell = self.lstm(input, (hidden, cell))\n",
    "            output = self.linear(hidden)\n",
    "            # output = self.softmax(output)\n",
    "            outputs.append(output)\n",
    "            if target_tensor is not None:\n",
    "                embedding = self.embed(target_tensor[:, i]).reshape((batch_size, self.embed_size)).to(context.device)\n",
    "                input = torch.cat([context, embedding], dim = 1).to(context.device)\n",
    "            else:\n",
    "                #add the embedding of the last prediction\n",
    "                input = torch.cat([context, self.embed(torch.argmax(output, dim = 1))], dim = 1).to(context.device)\n",
    "        # info(f\"Outputs: {outputs}\")\n",
    "        return torch.stack(outputs).to(context.device), hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code.\n",
    "- Dataloader automatically loads in batches. The data need not be modified by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    idx = 0\n",
    "    pb = tqdm(dataloader, desc=\"Batches\", leave=False)\n",
    "    for data in pb:\n",
    "        idx+=1\n",
    "        \n",
    "        # info(f\"----Batch {idx}----\")\n",
    "        \n",
    "        input_tensor, target_tensor = data['image'].to(device), data['formula'].to(device)\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        encoder_output = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_output)\n",
    "        \n",
    "        # print(encoder_output.device, 'My device')\n",
    "        \n",
    "        # print(f\"Decoder OutDim : {decoder_outputs.shape}, Target Tensor Dim: {target_tensor.shape}\")\n",
    "        # print(f\"Target tensor: {target_tensor[0][0]}\")\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pb.set_description(f\"Loss: {loss.item()}\")\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=1, plot_every=5):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss().to(device) #as stated in assignment\n",
    "    \n",
    "    # Print model's device\n",
    "    # print(\"Encoder's device:\", next(encoder.parameters()).device)\n",
    "\n",
    "    pb = tqdm(range(1, n_epochs + 1), desc=\"Epochs\")\n",
    "    for epoch in pb:\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            \n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "        \n",
    "        pb.set_description('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "        \n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "vocab_size = 1000\n",
    "CONTEXT_SIZE = 512\n",
    "HIDDEN_SIZE = 512\n",
    "EMBED_SIZE = 512\n",
    "MAX_EXAMPLES = 1000\n",
    "# image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Lambda(lambda x: x/255.0), #min-max normalisation\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Loaded Dataset <bound method DataFrame.info of                 image                                            formula\n",
      "0      74d337e8a0.png  $ \\gamma _ { \\Omega R , 5 } ^ { T } = - \\gamma...\n",
      "1      2d0f18f71d.png  $ l ^ { ( -- ) \\underline { { m } } } u _ { \\u...\n",
      "2      6d9b9de88d.png  $ \\left[ H , \\gamma _ { i } ^ { \\left( 2 \\righ...\n",
      "3      38c6d510bb.png  $ < a _ { i } > \\; \\propto \\; \\int _ { \\omega ...\n",
      "4      24537a86e3.png  $ \\Psi ( \\mu _ { 1 } , \\ldots , \\mu _ { K } ) ...\n",
      "...               ...                                                ...\n",
      "74995  1fa37e67d2.png  $ T _ { \\theta } ^ { \\theta } = - \\frac { 1 } ...\n",
      "74996  75518a26df.png  $ \\alpha _ { + } = - 1 / \\alpha _ { - } = \\sqr...\n",
      "74997  29f28cbc3a.png  $ d s ^ { 2 } = Z ^ { - 1 / 2 } \\eta _ { \\mu \\...\n",
      "74998  33ac7b385d.png  $ \\tilde { H } _ { 0 } = \\frac { 1 } { 2 } ( \\...\n",
      "74999  52672fbf76.png  $ \\psi _ { \\alpha \\beta } = - g _ { \\alpha \\ga...\n",
      "\n",
      "[75000 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "#part a\n",
    "#train_csv_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/train.csv\"\n",
    "#image_root_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/images\"\n",
    "train_csv_path = \"data/SyntheticData/train.csv\"\n",
    "image_root_path = \"data/SyntheticData/images\"\n",
    "train_dataloader, train_dataset = get_dataloader(train_csv_path, image_root_path, batch_size, transform, max_examples=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a network instance\n",
    "encoder = EncoderCNN().to(device)\n",
    "decoder = Decoder(CONTEXT_SIZE, train_dataset.vocab, n_layers=1, hidden_size= HIDDEN_SIZE, embed_size=EMBED_SIZE,max_length=train_dataset.maxlen).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166m 2s (- 16437m 36s) (1 1%) 0.7102:   1%|          | 1/100 [3:26:04<340:02:03, 12364.88s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(train_dataloader, encoder, decoder, \u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb Cell 22\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m pb \u001b[39m=\u001b[39m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpochs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m pb:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     print_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     plot_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m decoder_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m encoder_output \u001b[39m=\u001b[39m encoder(input_tensor)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m decoder_outputs, _, _ \u001b[39m=\u001b[39m decoder(encoder_output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# print(encoder_output.device, 'My device')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# print(f\"Decoder OutDim : {decoder_outputs.shape}, Target Tensor Dim: {target_tensor.shape}\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# print(f\"Target tensor: {target_tensor[0][0]}\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     decoder_outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, decoder_outputs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     target_tensor\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb Cell 22\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size))\u001b[39m.\u001b[39mto(context\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     hidden, cell \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, (hidden, cell))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X30sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39m# output = self.softmax(output)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1347\u001b[0m, in \u001b[0;36mLSTMCell.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     hx \u001b[39m=\u001b[39m (hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched \u001b[39melse\u001b[39;00m hx\n\u001b[0;32m-> 1347\u001b[0m ret \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm_cell(\n\u001b[1;32m   1348\u001b[0m     \u001b[39minput\u001b[39;49m, hx,\n\u001b[1;32m   1349\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_hh,\n\u001b[1;32m   1350\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_hh,\n\u001b[1;32m   1351\u001b[0m )\n\u001b[1;32m   1353\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched:\n\u001b[1;32m   1354\u001b[0m     ret \u001b[39m=\u001b[39m (ret[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m), ret[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_dataloader, encoder, decoder, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
