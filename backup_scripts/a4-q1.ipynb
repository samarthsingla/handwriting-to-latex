{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-11-13T16:50:28.772900Z","iopub.status.busy":"2023-11-13T16:50:28.772514Z","iopub.status.idle":"2023-11-13T16:50:43.604811Z","shell.execute_reply":"2023-11-13T16:50:43.603822Z","shell.execute_reply.started":"2023-11-13T16:50:28.772868Z"}},"source":["%pip install numpy==1.26.2"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T15:19:37.093589Z","iopub.status.busy":"2023-11-26T15:19:37.093200Z","iopub.status.idle":"2023-11-26T15:19:40.157039Z","shell.execute_reply":"2023-11-26T15:19:40.155936Z","shell.execute_reply.started":"2023-11-26T15:19:37.093559Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"data":{"text/plain":["<contextlib.ExitStack at 0x7f746f334130>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import logging\n","\n","debug = logging.getLogger(\"Debug\")\n","info  = print\n","plt.ion()   # interactive mode"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T15:19:40.159385Z","iopub.status.busy":"2023-11-26T15:19:40.158959Z","iopub.status.idle":"2023-11-26T15:19:40.209779Z","shell.execute_reply":"2023-11-26T15:19:40.208797Z","shell.execute_reply.started":"2023-11-26T15:19:40.159359Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CUDA Mode: cuda Tesla P100-PCIE-16GB\n"]}],"source":["#check GPU\n","device = None\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"Running CUDA Mode:\", device, torch.cuda.get_device_name(0))\n","elif torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","    print(\"Running MPS Mode:\", device)\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running CPU Mode:\", device)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data and Classes\n","- Create Dataloader class\n","\n","Note: Working on Part (a) as of now.  \n","Guiding light: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T15:19:40.211771Z","iopub.status.busy":"2023-11-26T15:19:40.211466Z","iopub.status.idle":"2023-11-26T15:19:40.235541Z","shell.execute_reply":"2023-11-26T15:19:40.234492Z","shell.execute_reply.started":"2023-11-26T15:19:40.211744Z"},"trusted":true},"outputs":[],"source":["START_TOKEN = \"<START>\"\n","END_TOKEN = \"<END>\"\n","UNK_TOKEN = \"<UNK>\"\n","PAD_TOKEN = \"<PAD>\"\n","# MAX_EXAMPLES = 100\n","class Vocabulary:\n","    def __init__(self, freq_dict, wd_to_id, id_to_wd):\n","        self.freq_dict = freq_dict\n","        self.wd_to_id = wd_to_id\n","        self.id_to_wd = id_to_wd\n","        self.N = len(freq_dict)\n","    \n","    def get_id(self, word):\n","        if word in self.wd_to_id:\n","            return self.wd_to_id[word]\n","        else:\n","            return self.wd_to_id[UNK_TOKEN]\n","    \n","    def decode(self, outputs, raw=True):\n","        \"\"\"\n","        Takes in raw logits(output) or formula and returns the decoded string\n","        \"\"\"\n","        if raw:\n","            outputs = torch.argmax(outputs, dim = 1)\n","        outputs = outputs.tolist()\n","        # print(outputs)\n","        formula = ' '.join([self.id_to_wd[id] for id in outputs])\n","        return formula\n","    \n","class LatexFormulaDataset(Dataset):\n","    \"\"\"Latex Formula Dataset: Image and Text\"\"\"\n","    \n","    def __init__(self, csv_file, root_dir, max_examples=None, transform = None):\n","        \"\"\"\n","        Arguments:\n","            csv_file (string): Path to the csv file with image name and text\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        #@TODO: May want to preload images\n","        info(\"Loading Dataset...\")\n","        self.df = pd.read_csv(csv_file)\n","        info(\"Loaded Dataset\", self.df.info)\n","        \n","        #Slice the dataset if max_examples is not None\n","        if max_examples is not None:\n","            self.df = self.df.iloc[:max_examples, :]\n","\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","        self.df['formula'] = self.df['formula'].apply(lambda x: x.split())\n","        self.df['formula'] = self.df['formula'].apply(lambda x: [START_TOKEN] + x + [END_TOKEN])\n","\n","        self.maxlen = 0\n","        for formula in self.df['formula']:\n","            if len(formula) > self.maxlen:\n","                self.maxlen = len(formula)\n","        \n","        def convert_to_ids(formula):\n","            form2 = []\n","            for wd in formula:\n","                form2.append(self.vocab.get_id(wd))\n","            form2 = torch.tensor(form2, dtype=torch.int64)\n","            return form2\n","        \n","        self.df['formula'] = self.df['formula'].apply(lambda x: x +[PAD_TOKEN]*(self.maxlen - len(x)))\n","        self.vocab= self.construct_vocab() \n","        self.df['formula'] = self.df['formula'].apply(convert_to_ids)\n","        \n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns sample of type image, textformula\n","        \"\"\"\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,\n","                                self.df.iloc[idx, 0])\n","        image = io.imread(img_name)\n","        formula = self.df.iloc[idx, 1]\n","\n","        # formula = np.array([formula], dtype=str).reshape(-1, 1)\n","        # formula = [self.vocab.get_id(wd[0]) for wd in formula]\n","        \n","        sample = {'image': image, 'formula': formula}\n","\n","        if self.transform:\n","            sample['image'] = self.transform(sample['image'])\n","            \n","        return sample \n","    \n","    def construct_vocab(self):\n","        \"\"\"\n","        Constructs vocabulary from the dataset formulas\n","        \"\"\"\n","        #Split on spaces to tokenize\n","        freq_dict = {}\n","        for formula in self.df['formula']:\n","            for wd in formula:\n","                if wd not in freq_dict:\n","                    freq_dict[wd] = 1\n","                else:\n","                    freq_dict[wd] += 1\n","        freq_dict[UNK_TOKEN] = 1\n","        N = len(freq_dict)\n","        wd_to_id = {}\n","        for i, wd in enumerate(freq_dict):\n","            wd_to_id[wd] = i\n","        id_to_wd = {v: k for k, v in wd_to_id.items()}\n","    \n","        #pad the formulas with \n","        return Vocabulary(freq_dict, wd_to_id, id_to_wd)      \n","\n","def get_dataloader(csv_path, image_root, batch_size, transform = None, max_examples = None):\n","    \"\"\"\n","    Returns dataloader,dataset for the dataset\n","    \"\"\"\n","    dataset = LatexFormulaDataset(csv_path, image_root, max_examples=max_examples,transform=transform) #checked\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    return dataloader, dataset\n","     "]},{"cell_type":"markdown","metadata":{},"source":["### Model Components\n","- A CNN to encode image to more meaningful vector"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T15:19:40.237713Z","iopub.status.busy":"2023-11-26T15:19:40.237328Z","iopub.status.idle":"2023-11-26T15:19:40.264173Z","shell.execute_reply":"2023-11-26T15:19:40.263301Z","shell.execute_reply.started":"2023-11-26T15:19:40.237679Z"},"trusted":true},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","    \n","        #@TODO:reduce number of layers: eliminate pools and acts\n","        self.conv1 = nn.Conv2d(3, 32, (5, 5))\n","        self.act1 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv2 = nn.Conv2d(32, 64, (5, 5))\n","        self.act2 = nn.ReLU()\n","        self.pool2 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv3 = nn.Conv2d(64, 128, (5, 5))\n","        self.act3 = nn.ReLU()\n","        self.pool3 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv4 = nn.Conv2d(128, 256, (5, 5))\n","        self.act4 = nn.ReLU()\n","        self.pool4 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv5 = nn.Conv2d(256, 512, (5, 5))\n","        self.act5 = nn.ReLU()\n","        self.pool5 = nn.MaxPool2d((2, 2))\n","        \n","        self.avg_pool = nn.AvgPool2d((3, 3))\n","    \n","    def forward(self, x):\n","        x = self.act1(self.conv1(x))\n","        x = self.pool1(x)\n","        \n","        x = self.act2(self.conv2(x))\n","        x = self.pool2(x)\n","        \n","        x = self.act3(self.conv3(x))\n","        x = self.pool3(x)\n","        \n","        x = self.act4(self.conv4(x))\n","        x = self.pool4(x)\n","        \n","        x = self.act5(self.conv5(x))\n","        x = self.pool5(x)\n","        \n","        x = self.avg_pool(x)\n","        x = x.view(-1,512) \n","        # info(f\"Encoder Output Shape: {x.shape}\")\n","        return x\n","    \n","class Decoder(nn.Module):\n","    \"\"\"\n","    Inputs:\n","    (here M is whatever the batch size is passed)\n","\n","    context_size : size of the context vector [shape: (1,M,context_size)]\n","    n_layers: number of layers [for our purposes, defaults to 1]\n","    hidden_size : size of the hidden state vectors [shape: (n_layers,M,hidden_size)]\n","    embed_size : size of the embedding vectors [shape: (1,M,embed_size)]\n","    vocab_size : size of the vocabulary\n","    max_length : maximum length of the formula\n","    \"\"\"\n","    def __init__(self, context_size, vocab, n_layers = 1, hidden_size = 512, embed_size = 512,  max_length = 100):\n","        super().__init__()\n","        self.context_size = context_size\n","        self.vocab = vocab\n","        self.vocab_size = vocab.N\n","        self.n_layers = n_layers\n","        self.hidden_size = hidden_size\n","        self.embed_size = embed_size\n","        self.max_length = max_length\n","        self.input_size = context_size + embed_size\n","\n","        self.embed = nn.Embedding(self.vocab_size, embed_size)\n","        self.lstm = nn.LSTMCell(self.input_size, self.hidden_size)\n","        self.linear = nn.Linear(hidden_size, self.vocab_size)\n","        self.softmax = nn.Softmax(dim = 1)\n","    \n","    def forward(self, context, target_tensor = None):\n","        \"\"\"\n","        M: batch_size\n","        context is the context vector from the encoder [shape: (M,context_size)]\n","        target_tensor is the formula in tensor form [shape: (M,max_length)] (in the second dimension, it is sequence of indices of formula tokens)\n","            if target_tensor is not None, then we are in Teacher Forcing mode\n","            else normal jo bhi (last prediction ka embed is concatenated)\n","        \"\"\"\n","        # info(\"Decoder Forward\")\n","        # info(f\"Context shape: {context.shape}\")\n","        context.to(device)\n","        batch_size = context.shape[0]\n","\n","        #initialize hidden state and cell state\n","            #@TODO: Some caveat in the size of the cell state. Should it be same as hidden_size? (check nn.LSTM documentation)\n","        hidden = context\n","        cell = torch.zeros((batch_size, self.hidden_size)).to(context.device)\n","\n","        #initialize the input with embedding of the start token\n","        init_embed = self.embed(torch.tensor([self.vocab.wd_to_id[START_TOKEN]]).to(device)).reshape((1, self.embed_size))\n","        init_embed = torch.repeat_interleave(init_embed, batch_size, dim = 0).to(context.device)\n","\n","        # info(f\"Initial Embedding Shape: {init_embed.shape}\")\n","\n","        input = torch.cat([context, init_embed], dim = 1).to(context.device)\n","\n","        #initialize the output_history and init_output\n","        outputs = []\n","        output = torch.zeros((batch_size, self.vocab_size)).to(context.device)\n","        \n","        for i in range(self.max_length):\n","            hidden, cell = self.lstm(input, (hidden, cell))\n","            output = self.linear(hidden)\n","            # output = self.softmax(output)\n","            outputs.append(output)\n","            \n","            #teacher forcing: 50% times\n","            r = torch.rand(1)\n","            if r>0.5 and target_tensor is not None:\n","                embedding = self.embed(target_tensor[:, i]).reshape((batch_size, self.embed_size)).to(context.device)\n","                input = torch.cat([context, embedding], dim = 1).to(context.device)           \n","            else:\n","                #add the embedding of the last prediction\n","                input = torch.cat([context, self.embed(torch.argmax(output, dim = 1))], dim = 1).to(context.device)\n","        # info(f\"Outputs: {outputs}\")\n","        return torch.stack(outputs).permute(1,0,2), hidden, cell"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-11-13T18:15:56.106481Z","iopub.status.busy":"2023-11-13T18:15:56.105734Z","iopub.status.idle":"2023-11-13T18:15:56.198416Z","shell.execute_reply":"2023-11-13T18:15:56.197693Z","shell.execute_reply.started":"2023-11-13T18:15:56.106446Z"}},"source":["### Vocabulary\n","- https://github.com/harvardnlp/im2markup/blob/master"]},{"cell_type":"markdown","metadata":{},"source":["### Complete Network"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T15:19:40.268051Z","iopub.status.busy":"2023-11-26T15:19:40.267497Z","iopub.status.idle":"2023-11-26T15:19:40.279102Z","shell.execute_reply":"2023-11-26T15:19:40.278187Z","shell.execute_reply.started":"2023-11-26T15:19:40.268024Z"},"trusted":true},"outputs":[],"source":["class HandwritingToLatex1(nn.Module):\n","    def __init__(self, context_size, vocab, n_layers = 1, hidden_size = 512, embed_size = 512,  max_length = 100):\n","        super().__init__()\n","        self.encoder = EncoderCNN()\n","        self.decoder = Decoder(context_size, vocab, n_layers, hidden_size, embed_size, max_length)\n","    \n","    def forward(self, image, target_tensor = None):\n","        context = self.encoder(image)\n","        outputs, hidden, cell = self.decoder(context, target_tensor)\n","        return outputs\n"]},{"cell_type":"markdown","metadata":{},"source":["### Utility Functions"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T15:19:40.280490Z","iopub.status.busy":"2023-11-26T15:19:40.280177Z","iopub.status.idle":"2023-11-26T15:19:40.295154Z","shell.execute_reply":"2023-11-26T15:19:40.294177Z","shell.execute_reply.started":"2023-11-26T15:19:40.280464Z"},"trusted":true},"outputs":[],"source":["import time\n","import math\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import numpy as np\n","from tqdm import tqdm\n","\n","plt.switch_backend('agg')\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # this locator puts ticks at regular intervals\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)\n","def saveModel(save_path, model_state, optimiser_state, loss):\n","    torch.save({\n","            'model_state_dict': model_state,\n","            'optimizer_state_dict':optimiser_state,\n","            'loss': loss,  \n","    }, save_path)"]},{"cell_type":"markdown","metadata":{},"source":["### Training Code.\n","- Dataloader automatically loads in batches. The data need not be modified by us."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T15:19:40.296944Z","iopub.status.busy":"2023-11-26T15:19:40.296538Z","iopub.status.idle":"2023-11-26T15:19:40.313487Z","shell.execute_reply":"2023-11-26T15:19:40.312575Z","shell.execute_reply.started":"2023-11-26T15:19:40.296888Z"},"trusted":true},"outputs":[],"source":["def train_epoch(dataloader,model, optimizer, criterion):\n","    total_loss = 0\n","    idx = 0\n","    pb = tqdm(dataloader, desc=\"Batch\")\n","    for data in pb:\n","        idx+=1\n","        # info(f\"----Batch {idx}----\")\n","        input_tensor, target_tensor = data['image'].to(device), data['formula'].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(input_tensor, target_tensor)\n","        output_logits = outputs.permute(0,2,1)\n","        loss = criterion(\n","            output_logits,\n","            target_tensor\n","        )\n","        loss.backward()\n","        optimizer.step() \n","\n","        \n","        if idx == 1:\n","            print(f\"---Start Sample---\")\n","            print(torch.argmax(outputs, dim=2).shape)\n","            train_dataset = dataloader.dataset\n","            if(train_dataset):\n","                generated_formula = [train_dataset.vocab.id_to_wd[token.item()] for token in torch.argmax(outputs, dim=2)[0]]\n","                required_formula = [train_dataset.vocab.id_to_wd[token.item()] for token in target_tensor[0]]\n","                print(f\"Generated: {' '.join(generated_formula)}\")\n","                print(f\"Actual: {' '.join(required_formula)}\")\n","            \n","\n","\n","        total_loss += loss.item()\n","        pb.set_description(f\"Batch {idx} Loss: {loss.item()}\")\n","\n","    return total_loss / len(dataloader)\n","\n","def train(train_dataloader, model, n_epochs, optimizer=None,learning_rate=0.001, print_every=1, plot_every=5, save_interval=2):\n","    start = time.time()\n","    model.train()\n","    \n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    if optimizer is None:\n","        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss(ignore_index=train_dataloader.dataset.vocab.wd_to_id[PAD_TOKEN]).to(device) #as stated in assignment\n","    # criterion = nn.CrossEntropyLoss().to(device) #as stated in assignment\n","    \n","    # Print model's device\n","    # print(\"Encoder's device:\", next(encoder.parameters()).device)\n","\n","    for epoch in range(1, n_epochs + 1):\n","        info(f\"Epoch {epoch}\")\n","\n","        \n","        loss = train_epoch(train_dataloader, model, optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if epoch % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            \n","        if epoch % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","        if epoch % save_interval == 0:\n","            saveModel(f'/kaggle/working/model_epoch_{epoch}.pt', model.state_dict(), optimizer.state_dict(), loss)    \n","            \n","#             saveModel(f'checkpoints/model_epoch_{epoch}.pt', model.state_dict(), optimizer.state_dict(), loss)    \n","                \n","        info('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg))\n","        \n","    return plot_losses"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T15:19:40.315566Z","iopub.status.busy":"2023-11-26T15:19:40.314806Z","iopub.status.idle":"2023-11-26T15:19:40.328459Z","shell.execute_reply":"2023-11-26T15:19:40.327632Z","shell.execute_reply.started":"2023-11-26T15:19:40.315529Z"},"trusted":true},"outputs":[],"source":["\n","batch_size = 32\n","vocab_size = 1000\n","CONTEXT_SIZE = 512\n","HIDDEN_SIZE = 512\n","EMBED_SIZE = 512\n","MAX_EXAMPLES = 1000\n","# image processing\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((224, 224)),\n","])"]},{"cell_type":"markdown","metadata":{},"source":["### Load Dataset and Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T15:19:40.329797Z","iopub.status.busy":"2023-11-26T15:19:40.329514Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading Dataset...\n","Loaded Dataset <bound method DataFrame.info of                 image                                            formula\n","0      74d337e8a0.png  $ \\gamma _ { \\Omega R , 5 } ^ { T } = - \\gamma...\n","1      2d0f18f71d.png  $ l ^ { ( -- ) \\underline { { m } } } u _ { \\u...\n","2      6d9b9de88d.png  $ \\left[ H , \\gamma _ { i } ^ { \\left( 2 \\righ...\n","3      38c6d510bb.png  $ < a _ { i } > \\; \\propto \\; \\int _ { \\omega ...\n","4      24537a86e3.png  $ \\Psi ( \\mu _ { 1 } , \\ldots , \\mu _ { K } ) ...\n","...               ...                                                ...\n","74995  1fa37e67d2.png  $ T _ { \\theta } ^ { \\theta } = - \\frac { 1 } ...\n","74996  75518a26df.png  $ \\alpha _ { + } = - 1 / \\alpha _ { - } = \\sqr...\n","74997  29f28cbc3a.png  $ d s ^ { 2 } = Z ^ { - 1 / 2 } \\eta _ { \\mu \\...\n","74998  33ac7b385d.png  $ \\tilde { H } _ { 0 } = \\frac { 1 } { 2 } ( \\...\n","74999  52672fbf76.png  $ \\psi _ { \\alpha \\beta } = - g _ { \\alpha \\ga...\n","\n","[75000 rows x 2 columns]>\n"]}],"source":["#part a\n","train_csv_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/train.csv\"\n","image_root_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/images\"\n","# train_csv_path = \"data/SyntheticData/train.csv\"\n","# image_root_path = \"data/SyntheticData/images\"\n","train_dataloader, train_dataset = get_dataloader(train_csv_path, image_root_path, batch_size, transform, max_examples=None)"]},{"cell_type":"markdown","metadata":{},"source":["### Create Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = HandwritingToLatex1(CONTEXT_SIZE, train_dataset.vocab, n_layers=1, hidden_size= HIDDEN_SIZE, embed_size=EMBED_SIZE,max_length=train_dataset.maxlen).to(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["losses = train(train_dataloader, model, 20, save_interval=2)"]},{"cell_type":"markdown","metadata":{},"source":["## Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def load_from_checkpoint(checkpoint_path):\n","    model_dicts = torch.load(checkpoint_path, map_location=device)\n","    model = HandwritingToLatex1(CONTEXT_SIZE, train_dataset.vocab, n_layers=1, hidden_size= HIDDEN_SIZE, embed_size=EMBED_SIZE,max_length=train_dataset.maxlen).to(device)\n","    model.load_state_dict(model_dicts['model_state_dict'])\n","    optims = torch.optim.Adam(model.parameters(), lr=0.001)\n","    optims.load_state_dict(model_dicts['optimizer_state_dict'])\n","    return model, optims, model_dicts['loss']\n","\n","# model_dicts = torch.load(\"checkpoints/encoder_epoch_4.pt\", map_location=device)\n","# print(model_dicts)\n","# model = HandwritingToLatex1(encoder, decoder).to(device)\n","# model.load_state_dict(model_dicts['model_state_dict'])\n","\n","# model_load, optims_load, loss = load_from_checkpoint(\"checkpoints/model2.0+12_epoch_4.pt\")\n","# print(loss)\n","#resume training from checkpoints\n","\n","# losses = train(train_dataloader, model_load, optimizer=optims_load,n_epochs=20, save_interval=2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":6947127,"sourceId":63599,"sourceType":"competition"}],"dockerImageVersionId":30558,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
