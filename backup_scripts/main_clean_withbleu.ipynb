{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CPU Mode: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "\n",
    "debug = logging.getLogger(\"Debug\")\n",
    "info  = print\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "#check GPU\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running CUDA Mode:\", device, torch.cuda.get_device_name(0))\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print(\"Running MPS Mode:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running CPU Mode:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "    \n",
    "def saveModel(save_path, model_state, optimiser_state, loss):\n",
    "    torch.save({\n",
    "            'model_state_dict': model_state,\n",
    "            'optimizer_state_dict':optimiser_state,\n",
    "            'loss': loss,  \n",
    "    }, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<START>\"\n",
    "END_TOKEN = \"<END>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_dict, wd_to_id, id_to_wd):\n",
    "        self.freq_dict = freq_dict\n",
    "        self.wd_to_id = wd_to_id\n",
    "        self.id_to_wd = id_to_wd\n",
    "        self.N = len(freq_dict)\n",
    "    \n",
    "    def get_id(self, word):\n",
    "        if word in self.wd_to_id:\n",
    "            return self.wd_to_id[word]\n",
    "        else:\n",
    "            return self.wd_to_id[UNK_TOKEN]\n",
    "\n",
    "    def decode(self, formula):\n",
    "        \"\"\"\n",
    "        Input shape: (seq_len,)\n",
    "        Output Shape: seq_len -> python list\n",
    "        \"\"\"\n",
    "        decoded = []\n",
    "        for wd in formula:\n",
    "            if wd in self.wd_to_id:\n",
    "                decoded.append(self.id_to_wd[wd])\n",
    "            else:\n",
    "                decoded.append(self.id_to_wd[UNK_TOKEN])\n",
    "\n",
    "\n",
    "class LatexFormulaDataset(Dataset):\n",
    "    \"\"\"Latex Formula Dataset: Image and Text\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform = None, max_examples = None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with image name and text\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        info(\"Loading Dataset...\")\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        info(\"Loaded.\")\n",
    "        #info(\"Loaded Dataset\", self.df.info)\n",
    "        \n",
    "        #Slice the dataset if max_examples is not None\n",
    "        if max_examples is not None:\n",
    "            self.df = self.df.iloc[:max_examples, :]\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.df['formula'] = self.df['formula'].apply(lambda x: x.split())\n",
    "        self.df['formula'] = self.df['formula'].apply(lambda x: [START_TOKEN] + x + [END_TOKEN])\n",
    "\n",
    "        self.maxlen = 0\n",
    "        for formula in self.df['formula']:\n",
    "            if len(formula) > self.maxlen:\n",
    "                self.maxlen = len(formula)\n",
    "        \n",
    "        def convert_to_ids(formula):\n",
    "            form2 = [self.vocab.get_id(wd) for wd in formula]\n",
    "            return torch.tensor(form2, dtype=torch.int64)\n",
    "        \n",
    "        self.df['formula'] = self.df['formula'].apply(lambda x: x +[PAD_TOKEN]*(self.maxlen - len(x)))\n",
    "        self.vocab= self.construct_vocab() \n",
    "        self.df['formula'] = self.df['formula'].apply(convert_to_ids)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns sample of type image, textformula\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.df.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        formula = self.df.iloc[idx, 1]\n",
    "\n",
    "        # formula = np.array([formula], dtype=str).reshape(-1, 1)\n",
    "        # formula = [self.vocab.get_id(wd[0]) for wd in formula]\n",
    "        \n",
    "        sample = {'image': image, 'formula': formula}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            \n",
    "        return sample \n",
    "    \n",
    "    def construct_vocab(self):\n",
    "        \"\"\"\n",
    "        Constructs vocabulary from the dataset formulas\n",
    "        \"\"\"\n",
    "        #Split on spaces to tokenize\n",
    "        freq_dict = {}\n",
    "        for formula in self.df['formula']:\n",
    "            for wd in formula:\n",
    "                if wd not in freq_dict:\n",
    "                    freq_dict[wd] = 1\n",
    "                else:\n",
    "                    freq_dict[wd] += 1\n",
    "        freq_dict[UNK_TOKEN] = 1\n",
    "        N = len(freq_dict)\n",
    "        wd_to_id = {}\n",
    "        for i, wd in enumerate(freq_dict):\n",
    "            wd_to_id[wd] = i\n",
    "        id_to_wd = {v: k for k, v in wd_to_id.items()}\n",
    "    \n",
    "        #pad the formulas with \n",
    "        return Vocabulary(freq_dict, wd_to_id, id_to_wd)      \n",
    "\n",
    "def get_dataloader(csv_path, image_root, batch_size, transform = None, max_examples = None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Returns dataloader,dataset for the dataset\n",
    "    \"\"\"\n",
    "    dataset = LatexFormulaDataset(csv_path, image_root, max_examples=max_examples,transform=transform) #checked\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4)\n",
    "    return dataloader, dataset\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        #@TODO:reduce number of layers: eliminate pools and acts\n",
    "        self.conv1 = nn.Conv2d(3, 32, (5, 5))       \n",
    "        self.conv2 = nn.Conv2d(32, 64, (5, 5))\n",
    "        self.conv3 = nn.Conv2d(64, 128, (5, 5))        \n",
    "        self.conv4 = nn.Conv2d(128, 256, (5, 5))        \n",
    "        self.conv5 = nn.Conv2d(256, 512, (5, 5))\n",
    "        \n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "        self.avg_pool = nn.AvgPool2d((3, 3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(-1,512) \n",
    "        # info(f\"Encoder Output Shape: {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "class DecoderLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    (here M is whatever the batch size is passed)\n",
    "\n",
    "    context_size : size of the context vector [shape: (1,M,context_size)]\n",
    "    n_layers: number of layers [for our purposes, defaults to 1]\n",
    "    hidden_size : size of the hidden state vectors [shape: (n_layers,M,hidden_size)]\n",
    "    embed_size : size of the embedding vectors [shape: (1,M,embed_size)]\n",
    "    vocab_size : size of the vocabulary\n",
    "    max_length : maximum length of the formula\n",
    "    \"\"\"\n",
    "    def __init__(self, context_size, vocab, max_seq_len, n_layers = 1, hidden_size = 512, embed_size = 512):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab.N\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.input_size = context_size + embed_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.embed = nn.Embedding(self.vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, self.vocab_size)\n",
    "    \n",
    "    def forward(self, context, target_tensor = None):\n",
    "        \"\"\"\n",
    "        M: batch_size\n",
    "        context is the context vector from the encoder [shape: (M,context_size)]\n",
    "        target_tensor is the formula in tensor form [shape: (M,max_length)] (in the second dimension, it is sequence of indices of formula tokens)\n",
    "            if target_tensor is not None, then we are in Teacher Forcing mode\n",
    "            else normal jo bhi (last prediction ka embed is concatenated)\n",
    "        \"\"\"\n",
    "        context.to(device)\n",
    "        batch_size = context.shape[0]\n",
    "\n",
    "        #initialize hidden state and cell state\n",
    "        hidden = context\n",
    "        cell = torch.zeros(batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        #initialize the input with embedding of the start token. Expand for batch size.\n",
    "        init_embed = self.embed(torch.tensor([self.vocab.wd_to_id[START_TOKEN]]).to(device).expand(batch_size, -1)).squeeze()\n",
    "        \n",
    "        #initialize the output_history and init_output\n",
    "        outputs = []\n",
    "        output = torch.zeros((batch_size, self.vocab_size)).to(device)\n",
    "        \n",
    "        \n",
    "        for i in range(self.max_seq_len):\n",
    "            #teacher forcing: 50% times\n",
    "            r = torch.rand(1)\n",
    "            if r>0.5 and target_tensor is not None:\n",
    "                if i==0 :\n",
    "                    embedding = init_embed\n",
    "                else: \n",
    "                    embedding = self.embed(target_tensor[:, i-1]).reshape((batch_size, self.embed_size)).to(device)            \n",
    "            else:\n",
    "                if i==0 :\n",
    "                    embedding = init_embed\n",
    "                else:\n",
    "                    #create embedding from previous input\n",
    "                    embedding = self.embed(torch.argmax(output, dim = 1))\n",
    "\n",
    "            lstm_input = torch.cat([context, embedding], dim = 1).to(device)\n",
    "    \n",
    "            hidden, cell = self.lstm(lstm_input, (hidden, cell))\n",
    "            output = self.linear(hidden)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        output_tensor = torch.stack(outputs).permute(1,0,2) #LBV - > BLV\n",
    "\n",
    "        return output_tensor, hidden, cell\n",
    "\n",
    "class HandwritingToLatexModel(nn.Module):\n",
    "    def __init__(self, context_size, vocab, max_seq_len, n_layers, hidden_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN()\n",
    "        self.decoder = DecoderLSTM(context_size, vocab, max_seq_len, n_layers, hidden_size, embed_size)\n",
    "    \n",
    "    def forward(self, image, target_tensor = None):\n",
    "        context = self.encoder(image)\n",
    "        outputs, hidden, cell = self.decoder(context, target_tensor)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader,model, optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    idx = 0\n",
    "    pb = tqdm(dataloader, desc=\"Batch\")\n",
    "    for data in pb:\n",
    "        idx+=1\n",
    "\n",
    "        input_tensor, target_tensor = data['image'].to(device), data['formula'].to(device)\n",
    "        outputs = model(input_tensor, target_tensor)\n",
    "        train_dataset = dataloader.dataset \n",
    "        if(train_dataset and idx%500==0):\n",
    "            generated_formula = [train_dataset.vocab.id_to_wd[token.item()] for token in torch.argmax(outputs, dim=2)[0]]\n",
    "            required_formula = [train_dataset.vocab.id_to_wd[token.item()] for token in target_tensor[0]]\n",
    "            print(f\"Generated: {' '.join(generated_formula)}\")\n",
    "            print(f\"Actual: {' '.join(required_formula)}\")\n",
    "\n",
    "        output_logits = outputs.permute(0,2,1)\n",
    "        \n",
    "        loss = criterion(\n",
    "            output_logits,\n",
    "            target_tensor\n",
    "        )\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pb.set_description(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train(train_dataloader, model, n_epochs, optimizer = None, learning_rate=0.001, print_every=1, save_interval=2, save_prefix = 'model'):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    if not optimizer: optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=train_dataloader.dataset.vocab.wd_to_id[PAD_TOKEN]).to(device) #as stated in assignment\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        info(f\"Epoch {epoch}\")\n",
    "        loss = train_epoch(train_dataloader, model, optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "    \n",
    "        if epoch % save_interval == 0:\n",
    "            saveModel(f'/kaggle/working/{save_prefix}_epoch_{epoch}.pt', model.state_dict(), optimizer.state_dict(), loss)    \n",
    "                \n",
    "        info('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "vocab_size = 1000\n",
    "CONTEXT_SIZE = 512\n",
    "HIDDEN_SIZE = 512\n",
    "EMBED_SIZE = 512\n",
    "MAX_EXAMPLES = 1000\n",
    "# image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "#part a\n",
    "train_csv_path = \"data/SyntheticData/train.csv\"\n",
    "image_root_path = \"data/SyntheticData/images/\"\n",
    "train_dataloader, train_dataset = get_dataloader(train_csv_path, image_root_path, batch_size, transform, max_examples=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandwritingToLatexModel(CONTEXT_SIZE, train_dataset.vocab, n_layers=1, hidden_size= HIDDEN_SIZE, embed_size=EMBED_SIZE, max_seq_len=train_dataset.maxlen).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:   0%|          | 0/2344 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/samarth/.pyenv/versions/3.10.4/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/samarth/.pyenv/versions/3.10.4/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'LatexFormulaDataset' on <module '__main__' (built-in)>\n",
      "Batch:   0%|          | 0/2344 [00:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(train_dataloader, model, \u001b[39m20\u001b[39;49m, save_interval\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, save_prefix \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mmodel3.0\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X24sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X24sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     info(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X24sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_epoch(train_dataloader, model, optimizer, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X24sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     print_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X24sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb Cell 17\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pb \u001b[39m=\u001b[39m tqdm(dataloader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m pb:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     idx\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     input_tensor, target_tensor \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device), data[\u001b[39m'\u001b[39m\u001b[39mformula\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/utils/data/dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    437\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/utils/data/dataloader.py:386\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1039\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1032\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1039\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1040\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1041\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39;49mwrite(fp\u001b[39m.\u001b[39;49mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_dataloader, model, 20, save_interval=2, save_prefix = 'model3.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading And Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_checkpoint(checkpoint_path):\n",
    "    model_dicts = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model = HandwritingToLatexModel(CONTEXT_SIZE, train_dataset.vocab, n_layers=1, hidden_size= HIDDEN_SIZE, embed_size=EMBED_SIZE, max_seq_len=train_dataset.maxlen).to(device)\n",
    "    model.load_state_dict(model_dicts['model_state_dict'])\n",
    "    \n",
    "    optims = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    optims.load_state_dict(model_dicts['optimizer_state_dict'])\n",
    "\n",
    "    return model, optims\n",
    "\n",
    "# model, optim = load_from_checkpoint(\"checkpoints/model3.0+6_epoch_12.pt\")\n",
    "\n",
    "#resume training from checkpoints\n",
    "#losses = train(train_dataloader, model, 20, optimizer = optim, save_interval=2, save_prefix = 'model2.0+18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "class BLEU(object):\n",
    "    @staticmethod\n",
    "    def compute(candidate, references, weights):\n",
    "        candidate = [c.lower() for c in candidate]\n",
    "        references = [[r.lower() for r in reference] for reference in references]\n",
    "\n",
    "        p_ns = (BLEU.modified_precision(candidate, references, i) for i, _ in enumerate(weights, start=1))\n",
    "        s = math.fsum(w * math.log(p_n) for w, p_n in zip(weights, p_ns) if p_n)\n",
    "\n",
    "        bp = BLEU.brevity_penalty(candidate, references)\n",
    "        return bp * math.exp(s)\n",
    "\n",
    "    @staticmethod\n",
    "    def modified_precision(candidate, references, n):\n",
    "        counts = Counter(ngrams(candidate, n))\n",
    "\n",
    "        if not counts:\n",
    "            return 0\n",
    "\n",
    "        max_counts = {}\n",
    "        for reference in references:\n",
    "            reference_counts = Counter(ngrams(reference, n))\n",
    "            for ngram in counts:\n",
    "                max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n",
    "\n",
    "        clipped_counts = dict((ngram, min(count, max_counts[ngram])) for ngram, count in counts.items())\n",
    "\n",
    "        return sum(clipped_counts.values()) / sum(counts.values())\n",
    "    \n",
    "    @staticmethod\n",
    "    def brevity_penalty(candidate, references):\n",
    "        c = len(candidate)\n",
    "        # r = min(abs(len(r) - c) for r in references)\n",
    "        r = min(len(r) for r in references)\n",
    "\n",
    "        if c > r:\n",
    "            return 1\n",
    "        else:\n",
    "            return math.exp(1 - r / c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv_path = \"data/SyntheticData/test.csv\"\n",
    "image_root_path = \"data/SyntheticData/images/\"\n",
    "test_dataloader, test_dataset = get_dataloader(test_csv_path, image_root_path, 32, transform, max_examples=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded, _ = load_from_checkpoint(\"checkpoints/model3.0+6_epoch_12.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the model\n",
    "import time\n",
    "\n",
    "model_loaded.eval()\n",
    "predictions = []\n",
    "gts = []\n",
    "# pb = tqdm.tqdm(test_dataloader, total=len(test_dataloader))\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    print(\"Entering Batch\")\n",
    "    input_tensor = batch['image'].to(device)\n",
    "    target_tensor = batch['formula'].to(device)\n",
    "    outputs = model_loaded(input_tensor)\n",
    "    outputs_final = torch.argmax(outputs, dim=2)\n",
    "    print(outputs_final.device)\n",
    "    for j in range(len(outputs)):\n",
    "        st = time.time()\n",
    "        # generated_formula = [train_dataset.vocab.id_to_wd[token.item()] for token in outputs_final[j]]\n",
    "        generated_formula = train_dataset.vocab.decode(outputs_final[j])\n",
    "        print(\"Time taken for decode\", time.time() - st)\n",
    "        # required_formula = [test_dataloader.dataset.vocab.id_to_wd[token.item()] for token in target_tensor[j]]\n",
    "        required_formula = test_dataloader.dataset.vocab.decode(target_tensor[j])\n",
    "        predictions.append(generated_formula)\n",
    "        gts.append(required_formula)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(predictions)\n",
    "totalBleu = 0\n",
    "for i in len(predictions):\n",
    "    gen_formula = predictions[i]\n",
    "    req_formula = gts[i]\n",
    "    gen_trim = []\n",
    "    for tken in gen_formula:\n",
    "        if tken == END_TOKEN:\n",
    "            break\n",
    "        gen_trim.append(tken)\n",
    "    req_trim = []\n",
    "    for tken in req_formula:\n",
    "        if tken == END_TOKEN:\n",
    "            break\n",
    "        req_trim.append(tken)\n",
    "    gen_formula = gen_trim[1:]\n",
    "    req_formula = req_trim[1:]\n",
    "    bleu = BLEU.compute(gen_formula, req_formula, [1/4, 1/4, 1/4, 1/4])\n",
    "    totalBleu += bleu\n",
    "\n",
    "print(f\"Macro Bleu Score: {totalBleu/N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
