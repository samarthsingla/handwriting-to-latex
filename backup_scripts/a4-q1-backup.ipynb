{"cells":[{"cell_type":"code","execution_count":379,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T16:50:28.772900Z","iopub.status.busy":"2023-11-13T16:50:28.772514Z","iopub.status.idle":"2023-11-13T16:50:43.604811Z","shell.execute_reply":"2023-11-13T16:50:43.603822Z","shell.execute_reply.started":"2023-11-13T16:50:28.772868Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy==1.22.4 in /Users/samarth/.pyenv/versions/3.10.4/lib/python3.10/site-packages (1.22.4)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install numpy==1.22.4"]},{"cell_type":"code","execution_count":380,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:59.678557Z","iopub.status.busy":"2023-11-13T17:51:59.678200Z","iopub.status.idle":"2023-11-13T17:51:59.688582Z","shell.execute_reply":"2023-11-13T17:51:59.687452Z","shell.execute_reply.started":"2023-11-13T17:51:59.678528Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<contextlib.ExitStack at 0x2e45d8910>"]},"execution_count":380,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import logging\n","\n","debug = logging.getLogger(\"Debug\")\n","info  = print\n","plt.ion()   # interactive mode"]},{"cell_type":"markdown","metadata":{},"source":["## Data and Classes\n","- Create Dataloader class\n","\n","Note: Working on Part (a) as of now.  \n","Guiding light: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"]},{"cell_type":"code","execution_count":381,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:34:51.523866Z","iopub.status.busy":"2023-11-13T17:34:51.523123Z","iopub.status.idle":"2023-11-13T17:34:51.532580Z","shell.execute_reply":"2023-11-13T17:34:51.531555Z","shell.execute_reply.started":"2023-11-13T17:34:51.523832Z"},"trusted":true},"outputs":[],"source":["START_TOKEN = \"START\"\n","END_TOKEN = \"END\"\n","UNK_TOKEN = \"UNK\"\n","\n","MAX_EXAMPLES = 100\n","class Vocabulary:\n","    def __init__(self, freq_dict, wd_to_id, id_to_wd):\n","        self.freq_dict = freq_dict\n","        self.wd_to_id = wd_to_id\n","        self.id_to_wd = id_to_wd\n","        self.N = len(freq_dict)\n","    \n","    def get_id(self, word):\n","        if word in self.wd_to_id:\n","            return self.wd_to_id[word]\n","        else:\n","            return self.wd_to_id[UNK_TOKEN]\n","\n","class LatexFormulaDataset(Dataset):\n","    \"\"\"Latex Formula Dataset: Image and Text\"\"\"\n","    \n","    def __init__(self, csv_file, root_dir, max_examples=None, transform = None):\n","        \"\"\"\n","        Arguments:\n","            csv_file (string): Path to the csv file with image name and text\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        #@TODO: May want to preload images\n","        self.df = pd.read_csv(csv_file)\n","\n","        if max_examples is not None:\n","            info(self.df.shape)\n","            self.df = self.df.iloc[:max_examples, :]\n","            info(self.df.shape)\n","            \n","        info(\"Loading Dataset\")\n","\n","        info(self.df.head())\n","        \n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","        '''Tokenize the formula by splitting on spaces'''\n","        self.df['formula'] = self.df['formula'].apply(lambda x: x.split())\n","        self.vocab= self.construct_vocab()  \n","\n","        self.maxlen = 0\n","        for formula in self.df['formula']:\n","            if len(formula) > self.maxlen:\n","                self.maxlen = len(formula)\n","\n","        self.df['formula'] = self.df['formula'].apply(lambda x: x + [END_TOKEN] + [UNK_TOKEN]*(self.maxlen - len(x)))\n","        self.maxlen += 1\n","        #slice df to first max_examples using iloc\n","\n","\n","            \n","        #Embedding layer\n","        self.embed = nn.Embedding(self.vocab.N, 512)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns sample of type image, textformula\n","        \"\"\"\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,\n","                                self.df.iloc[idx, 0])\n","        image = io.imread(img_name)\n","        formula = self.df.iloc[idx, 1]\n","        formula = np.array([formula], dtype=str).reshape(-1, 1)\n","        formula = [[self.vocab.get_id(wd[0]) for wd in formula]] \n","        sample = {'image': image, 'formula': torch.tensor(formula, dtype=torch.int64)}\n","\n","        if self.transform:\n","            sample['image'] = self.transform(sample['image'])\n","            \n","        return sample \n","    \n","    def construct_vocab(self):\n","        \"\"\"\n","        Constructs vocabulary from the dataset formulas\n","        \"\"\"\n","        freq_dict = {}\n","        for formula in self.df['formula']:\n","            for wd in formula:\n","                if wd not in freq_dict:\n","                    freq_dict[wd] = 1\n","                else:\n","                    freq_dict[wd] += 1\n","        freq_dict[START_TOKEN] = 1\n","        freq_dict[END_TOKEN] = 1\n","        freq_dict[UNK_TOKEN] = 1\n","        N = len(freq_dict)\n","        wd_to_id = {}\n","        for i, wd in enumerate(freq_dict):\n","            wd_to_id[wd] = i\n","        id_to_wd = {v: k for k, v in wd_to_id.items()}\n","    \n","        #pad the formulas with \n","        return Vocabulary(freq_dict, wd_to_id, id_to_wd)      \n","\n","def get_dataloader(csv_path, image_root, batch_size, transform = None, max_examples = None):\n","    \"\"\"\n","    Returns dataloader,dataset for the dataset\n","    \"\"\"\n","    dataset = LatexFormulaDataset(csv_path, image_root, max_examples=max_examples,transform=transform) #checked\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    return dataloader, dataset\n","     "]},{"cell_type":"markdown","metadata":{},"source":["### Encoder Network\n","- A CNN to encode image to more meaningful vector"]},{"cell_type":"code","execution_count":382,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T18:15:50.390411Z","iopub.status.busy":"2023-11-13T18:15:50.390048Z","iopub.status.idle":"2023-11-13T18:15:50.401998Z","shell.execute_reply":"2023-11-13T18:15:50.401089Z","shell.execute_reply.started":"2023-11-13T18:15:50.390380Z"},"trusted":true},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","    \n","        #@TODO:reduce number of layers: eliminate pools and acts\n","        self.conv1 = nn.Conv2d(3, 32, (5, 5))\n","        self.act1 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv2 = nn.Conv2d(32, 64, (5, 5))\n","        self.act2 = nn.ReLU()\n","        self.pool2 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv3 = nn.Conv2d(64, 128, (5, 5))\n","        self.act3 = nn.ReLU()\n","        self.pool3 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv4 = nn.Conv2d(128, 256, (5, 5))\n","        self.act4 = nn.ReLU()\n","        self.pool4 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv5 = nn.Conv2d(256, 512, (5, 5))\n","        self.act5 = nn.ReLU()\n","        self.pool5 = nn.MaxPool2d((2, 2))\n","        \n","        self.avg_pool = nn.AvgPool2d((3, 3))\n","    \n","    def forward(self, x):\n","        x = self.act1(self.conv1(x))\n","        x = self.pool1(x)\n","        \n","        x = self.act2(self.conv2(x))\n","        x = self.pool2(x)\n","        \n","        x = self.act3(self.conv3(x))\n","        x = self.pool3(x)\n","        \n","        x = self.act4(self.conv4(x))\n","        x = self.pool4(x)\n","        \n","        x = self.act5(self.conv5(x))\n","        x = self.pool5(x)\n","        \n","        x = self.avg_pool(x)\n","        x = x.view(-1,512) \n","        # info(f\"Encoder Output Shape: {x.shape}\")\n","        return x"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-11-13T18:15:56.106481Z","iopub.status.busy":"2023-11-13T18:15:56.105734Z","iopub.status.idle":"2023-11-13T18:15:56.198416Z","shell.execute_reply":"2023-11-13T18:15:56.197693Z","shell.execute_reply.started":"2023-11-13T18:15:56.106446Z"}},"source":["### Vocabulary\n","- https://github.com/harvardnlp/im2markup/blob/master"]},{"cell_type":"markdown","metadata":{},"source":["### Decoder Network"]},{"cell_type":"code","execution_count":383,"metadata":{},"outputs":[],"source":["class Decoder(nn.Module):\n","    \"\"\"\n","    Inputs:\n","    (here M is whatever the batch size is passed)\n","\n","    context_size : size of the context vector [shape: (1,M,context_size)]\n","    n_layers: number of layers [for our purposes, defaults to 1]\n","    hidden_size : size of the hidden state vectors [shape: (n_layers,M,hidden_size)]\n","    embed_size : size of the embedding vectors [shape: (1,M,embed_size)]\n","    vocab_size : size of the vocabulary\n","    max_length : maximum length of the formula\n","    \"\"\"\n","    def __init__(self, context_size, vocab, n_layers = 1, hidden_size = 512, embed_size = 512,  max_length = 100):\n","        super().__init__()\n","        self.context_size = context_size\n","        self.vocab = vocab\n","        self.vocab_size = vocab.N\n","        self.n_layers = n_layers\n","        self.hidden_size = hidden_size\n","        self.embed_size = embed_size\n","        self.max_length = max_length\n","\n","\n","        self.input_size = context_size + embed_size\n","\n","        self.embed = nn.Embedding(self.vocab_size, embed_size)\n","        self.lstm = nn.LSTMCell(self.input_size, self.hidden_size)\n","        self.linear = nn.Linear(hidden_size, self.vocab_size)\n","        self.softmax = nn.Softmax(dim = 1)\n","    \n","    def forward(self, context, target_tensor = None):\n","        \"\"\"\n","        M: batch_size\n","        context is the context vector from the encoder [shape: (M,context_size)]\n","        target_tensor is the formula in tensor form [shape: (M,max_length)] (in the second dimension, it is sequence of indices of formula tokens)\n","            if target_tensor is not None, then we are in Teacher Forcing mode\n","            else normal jo bhi (last prediction ka embed is concatenated)\n","        \"\"\"\n","        # info(\"Decoder Forward\")\n","        # info(f\"Context shape: {context.shape}\")\n","        \n","        batch_size = context.shape[0]\n","\n","        #initialize hidden state and cell state\n","            #@TODO: Some caveat in the size of the cell state. Should it be same as hidden_size? (check nn.LSTM documentation)\n","        hidden = torch.zeros((batch_size, self.hidden_size))\n","        cell = torch.zeros((batch_size, self.hidden_size))\n","\n","        #initialize the input with embedding of the start token\n","        init_embed = self.embed(torch.tensor([self.vocab.wd_to_id[START_TOKEN]])).reshape((1, self.embed_size))\n","        init_embed = torch.repeat_interleave(init_embed, batch_size, dim = 0)\n","\n","        # info(f\"Initial Embedding Shape: {init_embed.shape}\")\n","\n","        input = torch.cat([context, init_embed], dim = 1)\n","\n","        #initialize the output_history and init_output\n","        outputs = []\n","        output = torch.zeros((batch_size, self.vocab_size))\n","\n","        for i in range(self.max_length):\n","            hidden, cell = self.lstm(input, (hidden, cell))\n","            output = self.linear(hidden)\n","            # output = self.softmax(output)\n","            outputs.append(output)\n","            if target_tensor is not None:\n","                embedding = self.embed(target_tensor[:, i]).reshape((batch_size, self.embed_size))\n","                input = torch.cat([context, embedding], dim = 1)\n","            else:\n","                #add the embedding of the last prediction\n","                input = torch.cat([context, self.embed(torch.argmax(output, dim = 1))], dim = 1)\n","        # info(f\"Outputs: {outputs}\")\n","        return torch.stack(outputs), hidden, cell"]},{"cell_type":"markdown","metadata":{},"source":["### Utility Functions"]},{"cell_type":"code","execution_count":384,"metadata":{},"outputs":[],"source":["import time\n","import math\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import numpy as np\n","from tqdm import tqdm\n","\n","plt.switch_backend('agg')\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # this locator puts ticks at regular intervals\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)"]},{"cell_type":"markdown","metadata":{},"source":["### Training Code.\n","- Dataloader automatically loads in batches. The data need not be modified by us."]},{"cell_type":"code","execution_count":385,"metadata":{},"outputs":[],"source":["def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n","    total_loss = 0\n","    idx = 0\n","    for data in dataloader:\n","        idx+=1\n","        # info(f\"----Batch {idx}----\")\n","        input_tensor, target_tensor = data['image'], data['formula']\n","\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","\n","        encoder_output = encoder(input_tensor)\n","        decoder_outputs, _, _ = decoder(encoder_output)\n","\n","        loss = criterion(\n","            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n","            target_tensor.view(-1)\n","        )\n","        loss.backward()\n","\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(dataloader)\n","\n","def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=1, plot_every=5):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss() #as stated in assignment\n","\n","    for epoch in tqdm(range(1, n_epochs + 1)):\n","        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if epoch % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n","                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n","\n","        if epoch % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","    showPlot(plot_losses)"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":386,"metadata":{},"outputs":[],"source":["batch_size = 32\n","batch_size = MAX_EXAMPLES\n","\n","vocab_size = 1000\n","CONTEXT_SIZE = 512\n","HIDDEN_SIZE = 512\n","# OUTPUT_SIZE  = vocab_size\n","# MAX_LENGTH = 10000"]},{"cell_type":"code","execution_count":387,"metadata":{},"outputs":[],"source":["# image processing\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((224, 224)),\n","    transforms.Lambda(lambda x: x/255.0), #min-max normalisation\n","])"]},{"cell_type":"code","execution_count":388,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(75000, 2)\n","(100, 2)\n","Loading Dataset\n","            image                                            formula\n","0  74d337e8a0.png  $ \\gamma _ { \\Omega R , 5 } ^ { T } = - \\gamma...\n","1  2d0f18f71d.png  $ l ^ { ( -- ) \\underline { { m } } } u _ { \\u...\n","2  6d9b9de88d.png  $ \\left[ H , \\gamma _ { i } ^ { \\left( 2 \\righ...\n","3  38c6d510bb.png  $ < a _ { i } > \\; \\propto \\; \\int _ { \\omega ...\n","4  24537a86e3.png  $ \\Psi ( \\mu _ { 1 } , \\ldots , \\mu _ { K } ) ...\n","(100, 2)\n"]}],"source":["#part a\n","#train_csv_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/train.csv\"\n","#image_root_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/images\"\n","train_csv_path = \"data/SyntheticData/train.csv\"\n","image_root_path = \"data/SyntheticData/images\"\n","train_dataloader, train_dataset = get_dataloader(train_csv_path, image_root_path, batch_size, transform, max_examples=MAX_EXAMPLES)\n","vocab = train_dataset.vocab\n","MAX_LENGTH = train_dataset.maxlen\n","\n","print(train_dataset.df.shape)"]},{"cell_type":"code","execution_count":389,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 1/1000 [00:07<2:00:18,  7.23s/it]"]},{"name":"stdout","output_type":"stream","text":["0m 7s (- 120m 19s) (1 0%) 5.2621\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 2/1000 [00:14<1:58:30,  7.12s/it]"]},{"name":"stdout","output_type":"stream","text":["0m 14s (- 118m 46s) (2 0%) 4.8850\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 3/1000 [00:21<1:56:51,  7.03s/it]"]},{"name":"stdout","output_type":"stream","text":["0m 21s (- 117m 26s) (3 0%) 3.3599\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 4/1000 [00:28<1:55:44,  6.97s/it]"]},{"name":"stdout","output_type":"stream","text":["0m 28s (- 116m 32s) (4 0%) 2.6453\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 5/1000 [00:34<1:55:01,  6.94s/it]"]},{"name":"stdout","output_type":"stream","text":["0m 34s (- 115m 56s) (5 0%) 2.6303\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 6/1000 [00:41<1:54:32,  6.91s/it]"]},{"name":"stdout","output_type":"stream","text":["0m 41s (- 115m 29s) (6 0%) 2.4709\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 7/1000 [00:48<1:54:08,  6.90s/it]"]},{"name":"stdout","output_type":"stream","text":["0m 48s (- 115m 6s) (7 0%) 2.3081\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 8/1000 [00:55<1:54:09,  6.90s/it]"]},{"name":"stdout","output_type":"stream","text":["0m 55s (- 114m 55s) (8 0%) 2.3134\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 9/1000 [01:02<1:52:54,  6.84s/it]"]},{"name":"stdout","output_type":"stream","text":["1m 2s (- 114m 19s) (9 0%) 2.2730\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 10/1000 [01:09<1:52:18,  6.81s/it]"]},{"name":"stdout","output_type":"stream","text":["1m 9s (- 113m 54s) (10 1%) 2.2354\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 11/1000 [01:15<1:51:11,  6.75s/it]"]},{"name":"stdout","output_type":"stream","text":["1m 15s (- 113m 21s) (11 1%) 2.2204\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 12/1000 [01:22<1:50:56,  6.74s/it]"]},{"name":"stdout","output_type":"stream","text":["1m 22s (- 113m 1s) (12 1%) 2.2183\n"]},{"name":"stderr","output_type":"stream","text":["  1%|▏         | 13/1000 [01:28<1:50:20,  6.71s/it]"]},{"name":"stdout","output_type":"stream","text":["1m 29s (- 112m 37s) (13 1%) 2.2223\n"]},{"name":"stderr","output_type":"stream","text":["  1%|▏         | 14/1000 [01:35<1:50:36,  6.73s/it]"]},{"name":"stdout","output_type":"stream","text":["1m 35s (- 112m 25s) (14 1%) 2.2220\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 15/1000 [01:42<1:51:13,  6.78s/it]"]},{"name":"stdout","output_type":"stream","text":["1m 42s (- 112m 21s) (15 1%) 2.2233\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 16/1000 [01:49<1:50:25,  6.73s/it]"]},{"name":"stdout","output_type":"stream","text":["1m 49s (- 112m 1s) (16 1%) 2.2224\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 17/1000 [01:56<1:50:49,  6.76s/it]"]},{"name":"stdout","output_type":"stream","text":["1m 56s (- 111m 55s) (17 1%) 2.2231\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 18/1000 [02:02<1:50:24,  6.75s/it]"]},{"name":"stdout","output_type":"stream","text":["2m 2s (- 111m 41s) (18 1%) 2.2204\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 19/1000 [02:09<1:49:23,  6.69s/it]"]},{"name":"stdout","output_type":"stream","text":["2m 9s (- 111m 21s) (19 1%) 2.2175\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 20/1000 [02:16<1:49:14,  6.69s/it]"]},{"name":"stdout","output_type":"stream","text":["2m 16s (- 111m 8s) (20 2%) 2.2147\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 21/1000 [02:22<1:49:03,  6.68s/it]"]},{"name":"stdout","output_type":"stream","text":["2m 22s (- 110m 55s) (21 2%) 2.2136\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 22/1000 [02:29<1:49:45,  6.73s/it]"]},{"name":"stdout","output_type":"stream","text":["2m 29s (- 110m 50s) (22 2%) 2.2097\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 23/1000 [02:37<1:54:59,  7.06s/it]"]},{"name":"stdout","output_type":"stream","text":["2m 37s (- 111m 27s) (23 2%) 2.2099\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 24/1000 [02:45<1:59:27,  7.34s/it]"]},{"name":"stdout","output_type":"stream","text":["2m 45s (- 112m 7s) (24 2%) 2.2098\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▎         | 25/1000 [02:54<2:05:20,  7.71s/it]"]},{"name":"stdout","output_type":"stream","text":["2m 54s (- 113m 6s) (25 2%) 2.2062\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 26/1000 [03:02<2:06:46,  7.81s/it]"]},{"name":"stdout","output_type":"stream","text":["3m 2s (- 113m 39s) (26 2%) 2.2086\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 27/1000 [03:10<2:07:55,  7.89s/it]"]},{"name":"stdout","output_type":"stream","text":["3m 10s (- 114m 11s) (27 2%) 2.2075\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 28/1000 [03:18<2:12:04,  8.15s/it]"]},{"name":"stdout","output_type":"stream","text":["3m 18s (- 115m 4s) (28 2%) 2.2054\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 29/1000 [03:26<2:10:30,  8.06s/it]"]},{"name":"stdout","output_type":"stream","text":["3m 26s (- 115m 22s) (29 2%) 2.2061\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 30/1000 [03:34<2:08:58,  7.98s/it]"]},{"name":"stdout","output_type":"stream","text":["3m 34s (- 115m 36s) (30 3%) 2.2054\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 31/1000 [03:42<2:07:42,  7.91s/it]"]},{"name":"stdout","output_type":"stream","text":["3m 42s (- 115m 47s) (31 3%) 2.2059\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 32/1000 [03:50<2:07:36,  7.91s/it]"]},{"name":"stdout","output_type":"stream","text":["3m 50s (- 116m 2s) (32 3%) 2.2039\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 33/1000 [03:58<2:07:40,  7.92s/it]"]},{"name":"stdout","output_type":"stream","text":["3m 58s (- 116m 17s) (33 3%) 2.2026\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 34/1000 [04:05<2:06:37,  7.87s/it]"]},{"name":"stdout","output_type":"stream","text":["4m 5s (- 116m 25s) (34 3%) 2.2029\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▎         | 35/1000 [04:14<2:07:52,  7.95s/it]"]},{"name":"stdout","output_type":"stream","text":["4m 14s (- 116m 43s) (35 3%) 2.2046\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▎         | 36/1000 [04:21<2:07:23,  7.93s/it]"]},{"name":"stdout","output_type":"stream","text":["4m 21s (- 116m 52s) (36 3%) 2.2018\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▎         | 37/1000 [04:29<2:06:00,  7.85s/it]"]},{"name":"stdout","output_type":"stream","text":["4m 29s (- 116m 55s) (37 3%) 2.2039\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▎         | 37/1000 [04:31<1:57:42,  7.33s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1 tool.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m encoder \u001b[39m=\u001b[39m EncoderCNN()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m decoder \u001b[39m=\u001b[39m Decoder(CONTEXT_SIZE, vocab, n_layers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, hidden_size\u001b[39m=\u001b[39m HIDDEN_SIZE, embed_size\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m,max_length\u001b[39m=\u001b[39mMAX_LENGTH)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train(train_dataloader, encoder, decoder, \u001b[39m1000\u001b[39;49m)\n","\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1 tool.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss() \u001b[39m#as stated in assignment\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     print_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     plot_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n","\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1 tool.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m encoder_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m decoder_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m encoder_output \u001b[39m=\u001b[39m encoder(input_tensor)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m decoder_outputs, _, _ \u001b[39m=\u001b[39m decoder(encoder_output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     decoder_outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, decoder_outputs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     target_tensor\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n","File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1 tool.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool2(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact3(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool3(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X24sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact4(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv4(x))\n","File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#create a network instance\n","encoder = EncoderCNN()\n","decoder = Decoder(CONTEXT_SIZE, vocab, n_layers=1, hidden_size= HIDDEN_SIZE, embed_size=512,max_length=MAX_LENGTH)\n","train(train_dataloader, encoder, decoder, 1000)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
