{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    (here M is whatever the batch size is passed)\n",
    "\n",
    "    context_size : size of the context vector [shape: (1,M,context_size)]\n",
    "    n_layers: number of layers [for our purposes, defaults to 1]\n",
    "    hidden_size : size of the hidden state vectors [shape: (n_layers,M,hidden_size)]\n",
    "    embed_size : size of the embedding vectors [shape: (1,M,embed_size)]\n",
    "    vocab_size : size of the vocabulary\n",
    "    max_length : maximum length of the formula\n",
    "    \"\"\"\n",
    "    def __init__(self, context_size, vocab, n_layers = 1, hidden_size = 512, embed_size = 512,  max_length = 100):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab.N\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "\n",
    "        self.input_size = context_size + embed_size\n",
    "        self.embed = nn.Embedding(self.vocab_size, embed_size)\n",
    "        \n",
    "        self.lstm = nn.LSTMCell(self.input_size, hidden_size, n_layers)\n",
    "        self.linear = nn.Linear(hidden_size, self.vocab_size)\n",
    "        self.softmax = nn.Softmax(dim = 2)\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def forward(self, context, target_tensor = None):\n",
    "        \"\"\"\n",
    "        context is the context vector from the encoder [shape: (1,M,context_size)]\n",
    "        target_tensor is the formula in tensor form [shape: (1,M,max_length)] (in the second dimension, it is sequence of indices of formula tokens)\n",
    "            if target_tensor is not None, then we are in Teacher Forcing mode\n",
    "            else normal jo bhi (last prediction is concatenated)\n",
    "        \"\"\"\n",
    "        batch_size = context.shape[1]\n",
    "\n",
    "        #initialize hidden state and cell state\n",
    "            #@TODO: Some caveat in the size of the cell vector. Should it be same as hidden_size? (check nn.LSTM documentation)\n",
    "        hidden = torch.zeros((self.n_layers, batch_size, self.hidden_size))\n",
    "        cell = torch.zeros((self.n_layers, batch_size, self.hidden_size))\n",
    "\n",
    "        #initialize the input with embedding of the start token\n",
    "        init_embed = self.embed(torch.tensor([self.vocab.wd_to_id[START_TOKEN]])).reshape((1, batch_size, self.embed_size))\n",
    "        input = torch.cat([context, init_embed], dim = 2)\n",
    "\n",
    "        #initialize the output\n",
    "        output = torch.zeros((1, batch_size, self.vocab_size))\n",
    "\n",
    "        for i in range(self.max_length):\n",
    "            output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "            output = self.linear(output)\n",
    "            output = self.softmax(output)\n",
    "\n",
    "            \n",
    "            if target_tensor is not None:\n",
    "                input = torch.cat([context, self.embed(target_tensor[0, :, i]).reshape((1,batch_size, self.embed_size))], dim = 2)\n",
    "            else:\n",
    "                #add the embedding of the last prediction\n",
    "                input = torch.cat([context, self.embed(torch.argmax(output, dim = 2))], dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    context_size : size of the context vector\n",
    "    hidden_size : size of the hidden latent vectors\n",
    "    embed_size : literal\n",
    "    vocab_size : literal\n",
    "    output_size : one_hot?\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, context_size, hidden_size, embed_size, output_size, max_length):\n",
    "        super().__init__()\n",
    "\n",
    "        #class variables\n",
    "        self.embed_size = embed_size\n",
    "        self.context_size = context_size\n",
    "        self.max_length = max_length\n",
    "        self.vocab = vocab\n",
    "        vocab_size = vocab.N\n",
    "\n",
    "        #compute input size, concatenating context and prev. output embedding\n",
    "        input_size = context_size + embed_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers = 1)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size) #output_size = vocab_size\n",
    "    \n",
    "    def forward(self, context, target_tensor = None):\n",
    "        \"\"\"\n",
    "        target_tensor is of size MAX_LENGTH\n",
    "        \"\"\"\n",
    "        #START Token handling\n",
    "        batch_size = context.size(0)\n",
    "        start_id = self.vocab.get_id(START_TOKEN)\n",
    "        start_tensor = torch.empty(batch_size, 1, dtype = torch.int64).fill_(start_id)\n",
    "\n",
    "        decoder_input = torch.concatenate((context, self.embedding(start_tensor)), dim = 0)\n",
    "\n",
    "        print(f'Context shape: {context.shape}, decoder_input shape: {decoder_input.shape}')\n",
    "        print(f'embedding shape: {self.embedding(start_tensor).shape}')\n",
    "        print('====================================')\n",
    "\n",
    "        decoder_hidden = context  #dimensions are same\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(self.max_length):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                input_tensor = self.vocab.get_id(target_tensor[i])  #assuming target_tensor[i] is just a number\n",
    "                ground_truth_embed = self.embedding(input_tensor)\n",
    "                decoder_input = torch.concatenate((context, ground_truth_embed), dim = 0)\n",
    "            else:\n",
    "                #embed the last output, which was an index of vocab\n",
    "                last_out_embed = self.embedding(decoder_outputs[-1])\n",
    "                decoder_input = torch.concatenate((context, last_out_embed), dim = 0)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "        \n",
    "    def forward_step(self, input, hidden):\n",
    "        print('+++++++++++++++++++++++++=')\n",
    "        print(f'Input shape: {input.shape}, hidden shape: {hidden.shape}')\n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "        print(f'New hidden shape: {hidden.shape}')\n",
    "        output = self.out(hidden)\n",
    "\n",
    "        #get the output as just an index tensor\n",
    "        output = torch.argmax(output, dim = -1)\n",
    "\n",
    "        return output, hidden"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
