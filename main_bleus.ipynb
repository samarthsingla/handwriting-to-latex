{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:50:28.772900Z",
     "iopub.status.busy": "2023-11-13T16:50:28.772514Z",
     "iopub.status.idle": "2023-11-13T16:50:43.604811Z",
     "shell.execute_reply": "2023-11-13T16:50:43.603822Z",
     "shell.execute_reply.started": "2023-11-13T16:50:28.772868Z"
    }
   },
   "source": [
    "%pip install numpy==1.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T17:45:43.395205Z",
     "iopub.status.busy": "2023-11-26T17:45:43.394830Z",
     "iopub.status.idle": "2023-11-26T17:45:47.951475Z",
     "shell.execute_reply": "2023-11-26T17:45:47.950546Z",
     "shell.execute_reply.started": "2023-11-26T17:45:43.395170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x103c37100>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "\n",
    "debug = logging.getLogger(\"Debug\")\n",
    "info  = print\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T17:45:47.953504Z",
     "iopub.status.busy": "2023-11-26T17:45:47.953026Z",
     "iopub.status.idle": "2023-11-26T17:45:48.000046Z",
     "shell.execute_reply": "2023-11-26T17:45:47.999026Z",
     "shell.execute_reply.started": "2023-11-26T17:45:47.953477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MPS Mode: mps\n"
     ]
    }
   ],
   "source": [
    "#check GPU\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running CUDA Mode:\", device, torch.cuda.get_device_name(0))\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Running MPS Mode:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running CPU Mode:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Classes\n",
    "- Create Dataloader class\n",
    "\n",
    "Note: Working on Part (a) as of now.  \n",
    "Guiding light: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T17:45:48.002138Z",
     "iopub.status.busy": "2023-11-26T17:45:48.001614Z",
     "iopub.status.idle": "2023-11-26T17:45:48.023218Z",
     "shell.execute_reply": "2023-11-26T17:45:48.022126Z",
     "shell.execute_reply.started": "2023-11-26T17:45:48.002103Z"
    }
   },
   "outputs": [],
   "source": [
    "START_TOKEN = \"<START>\"\n",
    "END_TOKEN = \"<END>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_dict, wd_to_id, id_to_wd):\n",
    "        self.freq_dict = freq_dict\n",
    "        self.wd_to_id = wd_to_id\n",
    "        self.id_to_wd = id_to_wd\n",
    "        self.N = len(freq_dict)\n",
    "    \n",
    "    def get_id(self, word):\n",
    "        if word in self.wd_to_id:\n",
    "            return self.wd_to_id[word]\n",
    "        else:\n",
    "            return self.wd_to_id[UNK_TOKEN]\n",
    "        \n",
    "    def decode(self, formula):\n",
    "        \"\"\"\n",
    "        Input shape: (seq_len,)\n",
    "        Output Shape: seq_len -> python list\n",
    "        \"\"\"\n",
    "        return \" \".join([self.id_to_wd[idx.item()] for idx in formula])\n",
    "\n",
    "class LatexFormulaDataset(Dataset):\n",
    "    \"\"\"Latex Formula Dataset: Image and Text\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform = None, max_examples = None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with image name and text\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        info(\"Loading Dataset...\")\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        info(\"Loaded.\")\n",
    "        #info(\"Loaded Dataset\", self.df.info)\n",
    "        \n",
    "        #Slice the dataset if max_examples is not None\n",
    "        if max_examples is not None:\n",
    "            self.df = self.df.iloc[:max_examples, :]\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.df['formula'] = self.df['formula'].apply(lambda x: x.split())\n",
    "        self.df['formula'] = self.df['formula'].apply(lambda x: [START_TOKEN] + x + [END_TOKEN])\n",
    "\n",
    "        self.maxlen = 0\n",
    "        for formula in self.df['formula']:\n",
    "            if len(formula) > self.maxlen:\n",
    "                self.maxlen = len(formula)\n",
    "        \n",
    "        # def convert_to_ids(formula):\n",
    "        #     form2 = [self.vocab.get_id(wd) for wd in formula]\n",
    "        #     return torch.tensor(form2, dtype=torch.int64)\n",
    "        \n",
    "        self.df['formula'] = self.df['formula'].apply(lambda x: x +[PAD_TOKEN]*(self.maxlen - len(x)))\n",
    "        self.vocab= self.construct_vocab() \n",
    "        # self.df['formula'] = self.df['formula'].apply(convert_to_ids)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns sample of type image, textformula\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.df.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        formula = self.df.iloc[idx, 1]\n",
    "\n",
    "        # formula = np.array([formula], dtype=str).reshape(-1, 1)\n",
    "        # formula = [self.vocab.get_id(wd[0]) for wd in formula]\n",
    "        def convert_to_ids(formula):\n",
    "            form2 = [self.vocab.get_id(wd) for wd in formula]\n",
    "            return torch.tensor(form2, dtype=torch.int64)\n",
    "        \n",
    "        formula = convert_to_ids(formula)\n",
    "\n",
    "        sample = {'image': image, 'formula': formula}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            \n",
    "        return sample \n",
    "    \n",
    "    def construct_vocab(self):\n",
    "        \"\"\"\n",
    "        Constructs vocabulary from the dataset formulas\n",
    "        \"\"\"\n",
    "        #Split on spaces to tokenize\n",
    "        freq_dict = {}\n",
    "        for formula in self.df['formula']:\n",
    "            for wd in formula:\n",
    "                if wd not in freq_dict:\n",
    "                    freq_dict[wd] = 1\n",
    "                else:\n",
    "                    freq_dict[wd] += 1\n",
    "        freq_dict[UNK_TOKEN] = 1\n",
    "        N = len(freq_dict)\n",
    "        wd_to_id = {}\n",
    "        for i, wd in enumerate(freq_dict):\n",
    "            wd_to_id[wd] = i\n",
    "        #make id_to_wd a simple list\n",
    "        id_to_wd = [None]*N\n",
    "        for wd in wd_to_id:\n",
    "            id_to_wd[wd_to_id[wd]] = wd\n",
    "    \n",
    "        #pad the formulas with \n",
    "        return Vocabulary(freq_dict, wd_to_id, id_to_wd)      \n",
    "\n",
    "def get_dataloader(csv_path, image_root, batch_size, transform = None, max_examples = None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Returns dataloader,dataset for the dataset\n",
    "    \"\"\"\n",
    "    dataset = LatexFormulaDataset(csv_path, image_root, max_examples=max_examples,transform=transform) #checked\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader, dataset\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Network\n",
    "- A CNN to encode image to more meaningful vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T17:45:48.026284Z",
     "iopub.status.busy": "2023-11-26T17:45:48.025441Z",
     "iopub.status.idle": "2023-11-26T17:45:48.047135Z",
     "shell.execute_reply": "2023-11-26T17:45:48.046335Z",
     "shell.execute_reply.started": "2023-11-26T17:45:48.026256Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        #@TODO:reduce number of layers: eliminate pools and acts\n",
    "        self.conv1 = nn.Conv2d(3, 32, (5, 5))       \n",
    "        self.conv2 = nn.Conv2d(32, 64, (5, 5))\n",
    "        self.conv3 = nn.Conv2d(64, 128, (5, 5))        \n",
    "        self.conv4 = nn.Conv2d(128, 256, (5, 5))        \n",
    "        self.conv5 = nn.Conv2d(256, 512, (5, 5))\n",
    "        \n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "        self.avg_pool = nn.AvgPool2d((3, 3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(-1,512) \n",
    "        # info(f\"Encoder Output Shape: {x.shape}\")\n",
    "        return x\n",
    "    \n",
    "class DecoderLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    (here M is whatever the batch size is passed)\n",
    "\n",
    "    context_size : size of the context vector [shape: (1,M,context_size)]\n",
    "    n_layers: number of layers [for our purposes, defaults to 1]\n",
    "    hidden_size : size of the hidden state vectors [shape: (n_layers,M,hidden_size)]\n",
    "    embed_size : size of the embedding vectors [shape: (1,M,embed_size)]\n",
    "    vocab_size : size of the vocabulary\n",
    "    max_length : maximum length of the formula\n",
    "    \"\"\"\n",
    "    def __init__(self, context_size, vocab, max_seq_len, n_layers = 1, hidden_size = 512, embed_size = 512):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab.N\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.input_size = context_size + embed_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.embed = nn.Embedding(self.vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, self.vocab_size)\n",
    "    \n",
    "    def forward(self, context, target_tensor = None):\n",
    "        \"\"\"\n",
    "        M: batch_size\n",
    "        context is the context vector from the encoder [shape: (M,context_size)]\n",
    "        target_tensor is the formula in tensor form [shape: (M,max_length)] (in the second dimension, it is sequence of indices of formula tokens)\n",
    "            if target_tensor is not None, then we are in Teacher Forcing mode\n",
    "            else normal jo bhi (last prediction ka embed is concatenated)\n",
    "        \"\"\"\n",
    "        context.to(device)\n",
    "        batch_size = context.shape[0]\n",
    "\n",
    "        #initialize hidden state and cell state\n",
    "        hidden = context\n",
    "        cell = torch.zeros(batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        #initialize the input with embedding of the start token. Expand for batch size.\n",
    "        init_embed = self.embed(torch.tensor([self.vocab.wd_to_id[START_TOKEN]]).to(device).expand(batch_size, -1)).squeeze()\n",
    "        \n",
    "        #initialize the output_history and init_output\n",
    "        outputs = []\n",
    "        output = torch.zeros((batch_size, self.vocab_size)).to(device)\n",
    "        \n",
    "        \n",
    "        for i in range(self.max_seq_len):\n",
    "            #teacher forcing: 50% times\n",
    "            r = torch.rand(1)\n",
    "            if r>0.5 and target_tensor is not None:\n",
    "                if i==0 :\n",
    "                    embedding = init_embed\n",
    "                else: \n",
    "                    embedding = self.embed(target_tensor[:, i-1]).reshape((batch_size, self.embed_size)).to(device)            \n",
    "            else:\n",
    "                if i==0 :\n",
    "                    embedding = init_embed\n",
    "                else:\n",
    "                    #create embedding from previous input\n",
    "                    embedding = self.embed(torch.argmax(output, dim = 1))\n",
    "\n",
    "            lstm_input = torch.cat([context, embedding], dim = 1).to(device)\n",
    "    \n",
    "            hidden, cell = self.lstm(lstm_input, (hidden, cell))\n",
    "            output = self.linear(hidden)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        output_tensor = torch.stack(outputs).permute(1,0,2) #LBV - > BLV\n",
    "\n",
    "        return output_tensor, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T18:15:56.106481Z",
     "iopub.status.busy": "2023-11-13T18:15:56.105734Z",
     "iopub.status.idle": "2023-11-13T18:15:56.198416Z",
     "shell.execute_reply": "2023-11-13T18:15:56.197693Z",
     "shell.execute_reply.started": "2023-11-13T18:15:56.106446Z"
    }
   },
   "source": [
    "### Vocabulary\n",
    "- https://github.com/harvardnlp/im2markup/blob/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T17:45:48.048439Z",
     "iopub.status.busy": "2023-11-26T17:45:48.048162Z",
     "iopub.status.idle": "2023-11-26T17:45:48.062755Z",
     "shell.execute_reply": "2023-11-26T17:45:48.061890Z",
     "shell.execute_reply.started": "2023-11-26T17:45:48.048416Z"
    }
   },
   "outputs": [],
   "source": [
    "class HandwritingToLatexModel(nn.Module):\n",
    "    def __init__(self, context_size, vocab, max_seq_len, n_layers, hidden_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN()\n",
    "        self.decoder = DecoderLSTM(context_size, vocab, max_seq_len, n_layers, hidden_size, embed_size)\n",
    "    \n",
    "    def forward(self, image, target_tensor = None):\n",
    "        context = self.encoder(image)\n",
    "        outputs, hidden, cell = self.decoder(context, target_tensor)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T17:45:48.065258Z",
     "iopub.status.busy": "2023-11-26T17:45:48.064884Z",
     "iopub.status.idle": "2023-11-26T17:45:48.079212Z",
     "shell.execute_reply": "2023-11-26T17:45:48.078198Z",
     "shell.execute_reply.started": "2023-11-26T17:45:48.065224Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "    \n",
    "def saveModel(save_path, model_state, optimiser_state, loss):\n",
    "    torch.save({\n",
    "            'model_state_dict': model_state,\n",
    "            'optimizer_state_dict':optimiser_state,\n",
    "            'loss': loss,  \n",
    "    }, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code.\n",
    "- Dataloader automatically loads in batches. The data need not be modified by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T17:45:48.081574Z",
     "iopub.status.busy": "2023-11-26T17:45:48.080335Z",
     "iopub.status.idle": "2023-11-26T17:45:48.090046Z",
     "shell.execute_reply": "2023-11-26T17:45:48.089194Z",
     "shell.execute_reply.started": "2023-11-26T17:45:48.081543Z"
    }
   },
   "outputs": [],
   "source": [
    "def generated_formula(output, vocab):\n",
    "    \"\"\"\n",
    "    output: [shape: (Max_length,vocab_size)]\n",
    "    \"\"\"\n",
    "    output = torch.argmax(output, dim = 1)\n",
    "    output = output.tolist()\n",
    "    formula = ' '.join([vocab.id_to_wd[id] for id in output])\n",
    "    return formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:00:31.699279Z",
     "iopub.status.busy": "2023-11-26T18:00:31.698424Z",
     "iopub.status.idle": "2023-11-26T18:00:31.711976Z",
     "shell.execute_reply": "2023-11-26T18:00:31.710943Z",
     "shell.execute_reply.started": "2023-11-26T18:00:31.699250Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(dataloader,model, optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    idx = 0\n",
    "    pb = tqdm(dataloader, desc=\"Batch\")\n",
    "    for data in pb:\n",
    "        idx+=1\n",
    "\n",
    "        input_tensor, target_tensor = data['image'].to(device), data['formula'].to(device)\n",
    "        outputs = model(input_tensor, target_tensor)\n",
    "        train_dataset = dataloader.dataset \n",
    "        if(train_dataset and idx%500==0):\n",
    "            generated_formula = [train_dataset.vocab.id_to_wd[token.item()] for token in torch.argmax(outputs, dim=2)[0]]\n",
    "            required_formula = [train_dataset.vocab.id_to_wd[token.item()] for token in target_tensor[0]]\n",
    "            print(f\"Generated: {' '.join(generated_formula)}\")\n",
    "            print(f\"Actual: {' '.join(required_formula)}\")\n",
    "\n",
    "        output_logits = outputs.permute(0,2,1)\n",
    "        \n",
    "        loss = criterion(\n",
    "            output_logits,\n",
    "            target_tensor\n",
    "        )\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pb.set_description(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train(train_dataloader, model, n_epochs, optimizer = None, learning_rate=0.001, print_every=1, save_interval=2, save_prefix = 'model'):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    if not optimizer: optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=train_dataloader.dataset.vocab.wd_to_id[PAD_TOKEN]).to(device) #as stated in assignment\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        info(f\"Epoch {epoch}\")\n",
    "        loss = train_epoch(train_dataloader, model, optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "    \n",
    "        if epoch % save_interval == 0:\n",
    "            saveModel(f'/kaggle/working/{save_prefix}_epoch_{epoch}.pt', model.state_dict(), optimizer.state_dict(), loss)    \n",
    "                \n",
    "        info('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T18:00:32.221963Z",
     "iopub.status.busy": "2023-11-26T18:00:32.221588Z",
     "iopub.status.idle": "2023-11-26T18:00:32.227622Z",
     "shell.execute_reply": "2023-11-26T18:00:32.226283Z",
     "shell.execute_reply.started": "2023-11-26T18:00:32.221933Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "vocab_size = 1000\n",
    "CONTEXT_SIZE = 512\n",
    "HIDDEN_SIZE = 512\n",
    "EMBED_SIZE = 512\n",
    "MAX_EXAMPLES = 1000\n",
    "# image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "])\n",
    "transform_hw = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_PATH = \"data/\"\n",
    "MODEL_BASE_PATH = \"checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Requirement '/kaggle/input/nltk-version/nltk-3.8.1-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing /kaggle/input/nltk-version/nltk-3.8.1-py3-none-any.whl\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/nltk-version/nltk-3.8.1-py3-none-any.whl'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"/kaggle/input/nltk-version/nltk-3.8.1-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "print(nltk.__version__)\n",
    "def sbleu(GT,PRED):\n",
    "    score = 0\n",
    "    for i in tqdm(range(len(GT)), desc=\"Calculating BLEU Score\"):\n",
    "        Lgt = len(GT[i].split(' '))\n",
    "        if Lgt > 4 :\n",
    "            cscore = nltk.translate.bleu_score.sentence_bleu([GT[i].split(' ')],PRED[i].split(' '),weights=(0.25,0.25,0.25,0.25),smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "        else:\n",
    "            weight_lst = tuple([1.0/Lgt]*Lgt)\n",
    "            cscore = nltk.translate.bleu_score.sentence_bleu([GT[i].split(' ')],PRED[i].split(' '),weights=weight_lst,smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "        score += cscore\n",
    "    return score/(len(GT))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "train_csv_path = f\"{DATA_BASE_PATH}/SyntheticData/train.csv\"\n",
    "image_root_path = f\"data/SyntheticData/images/\"\n",
    "train_dataloader, train_dataset = get_dataloader(train_csv_path, image_root_path, batch_size, transform, max_examples=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "#TEST SYNTHETIC DATA\n",
    "test_csv_path = f\"{DATA_BASE_PATH}/SyntheticData/test.csv\"\n",
    "image_root_path = f\"{DATA_BASE_PATH}/SyntheticData/images/\"\n",
    "test_dataloader, test_dataset = get_dataloader(test_csv_path, image_root_path, 32, transform, max_examples=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "#VAL SYNTHETIC DATA\n",
    "val_csv_path = f\"{DATA_BASE_PATH}/SyntheticData/val.csv\"\n",
    "image_root_path = f\"{DATA_BASE_PATH}/SyntheticData/images/\"\n",
    "val_dataloader, test_dataset = get_dataloader(val_csv_path, image_root_path, 32, transform, max_examples=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "hw_train_csv_path = f\"{DATA_BASE_PATH}/HandwrittenData/val_hw.csv\"\n",
    "hw_image_root_path = f\"{DATA_BASE_PATH}/HandwrittenData/images/train/\"\n",
    "hw_val_dataloader, hw_train_dataset = get_dataloader(hw_train_csv_path, hw_image_root_path, batch_size, transform_hw, max_examples=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_checkpoint(checkpoint_path):\n",
    "    model_dicts = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model = HandwritingToLatexModel(CONTEXT_SIZE, train_dataset.vocab, n_layers=1, hidden_size= HIDDEN_SIZE, embed_size=EMBED_SIZE, max_seq_len=train_dataset.maxlen).to(device)\n",
    "    model.load_state_dict(model_dicts['model_state_dict'])\n",
    "    \n",
    "    optims = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    optims.load_state_dict(model_dicts['optimizer_state_dict'])\n",
    "\n",
    "    return model, optims\n",
    "\n",
    "# model, optim = load_from_checkpoint(\"checkpoints/model2.0+12_epoch_6.pt\")\n",
    "\n",
    "#resume training from checkpoints\n",
    "# losses = train(train_dataloader, model, 20, optimizer = optim, save_interval=2, save_prefix = 'model2.0+18')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded, _ = load_from_checkpoint(f\"{MODEL_BASE_PATH}model3.0_epoch_24.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the model\n",
    "import time\n",
    "def get_preds_gt(model, dataloader):\n",
    "    #Calculate predictions on dataloader\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    gts = []\n",
    "    pb = tqdm(dataloader, total=len(dataloader))\n",
    "    with torch.no_grad():\n",
    "        for batch in pb:\n",
    "            input_tensor = batch['image'].to(device)\n",
    "            target_tensor = batch['formula'].to(device)\n",
    "            outputs = model(input_tensor)\n",
    "            outputs_final = torch.argmax(outputs, dim=2)\n",
    "            for j in range(len(outputs)):\n",
    "                # generated_formula = [train_dataset.vocab.id_to_wd[token.item()] for token in outputs_final[j]]\n",
    "                generated_formula = train_dataset.vocab.decode(outputs_final[j]).split()\n",
    "                # required_formula = [test_dataloader.dataset.vocab.id_to_wd[token.item()] for token in target_tensor[j]]\n",
    "                required_formula = dataloader.dataset.vocab.decode(target_tensor[j]).split()\n",
    "                #Pre Process the formula\n",
    "                gen_trim = []\n",
    "                for wd in generated_formula:\n",
    "                    if wd == END_TOKEN:\n",
    "                        break\n",
    "                    gen_trim.append(wd)\n",
    "                generated_formula = \" \".join(gen_trim)\n",
    "                req_trim = []\n",
    "                for wd in required_formula:\n",
    "                    if wd == END_TOKEN:\n",
    "                        break\n",
    "                    req_trim.append(wd)\n",
    "                required_formula = \" \".join(req_trim)\n",
    "\n",
    "                predictions.append(generated_formula)\n",
    "                gts.append(required_formula)\n",
    "    return predictions, gts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/313 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pred, gt \u001b[39m=\u001b[39m get_preds_gt(model_loaded, val_dataloader)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(sbleu(gt, pred))\n",
      "\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m outputs_final \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outputs, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(outputs)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# generated_formula = [train_dataset.vocab.id_to_wd[token.item()] for token in outputs_final[j]]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     generated_formula \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mdecode(outputs_final[j])\u001b[39m.\u001b[39msplit()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# required_formula = [test_dataloader.dataset.vocab.id_to_wd[token.item()] for token in target_tensor[j]]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     required_formula \u001b[39m=\u001b[39m dataloader\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mdecode(target_tensor[j])\u001b[39m.\u001b[39msplit()\n",
      "\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, formula):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m    Input shape: (seq_len,)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m    Output Shape: seq_len -> python list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid_to_wd[idx\u001b[39m.\u001b[39mitem()] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m formula])\n",
      "\u001b[1;32m/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, formula):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m    Input shape: (seq_len,)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m    Output Shape: seq_len -> python list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samarth/Library/CloudStorage/OneDrive-IITDelhi/Sem5/COL774/assgns/assgn4/Handwriting-to-Latex-ML/main_bleus.ipynb#X43sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid_to_wd[idx\u001b[39m.\u001b[39;49mitem()] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m formula])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred, gt = get_preds_gt(model_loaded, val_dataloader)\n",
    "print(sbleu(gt, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1644.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7442820388799694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred, gt = get_preds_gt(model_loaded, hw_val_dataloader)\n",
    "print(sbleu(gt, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 279/279 [04:25<00:00,  1.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8907/8907 [00:18<00:00, 481.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6689380787742844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred, gt = get_preds_gt(model_loaded, test_dataloader)\n",
    "print(sbleu(gt, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6947127,
     "sourceId": 63599,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
