{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x16e098be0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "\n",
    "debug = logging.getLogger(\"Debug\")\n",
    "info  = print\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MPS Mode: mps\n"
     ]
    }
   ],
   "source": [
    "#check GPU\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running CUDA Mode:\", device, torch.cuda.get_device_name(0))\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Running MPS Mode:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running CPU Mode:\", device)\n",
    "\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "    \n",
    "def saveModel(save_path, model_state, optimiser_state, loss):\n",
    "    torch.save({\n",
    "            'model_state_dict': model_state,\n",
    "            'optimizer_state_dict':optimiser_state,\n",
    "            'loss': loss,  \n",
    "    }, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = None\n",
    "MAX_SEQ_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<START>\"\n",
    "END_TOKEN = \"<END>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_dict, wd_to_id, id_to_wd):\n",
    "        self.freq_dict = freq_dict\n",
    "        self.wd_to_id = wd_to_id\n",
    "        self.id_to_wd = id_to_wd\n",
    "        self.N = len(freq_dict)\n",
    "    \n",
    "    def get_id(self, word):\n",
    "        if word in self.wd_to_id:\n",
    "            return self.wd_to_id[word]\n",
    "        else:\n",
    "            return self.wd_to_id[UNK_TOKEN]\n",
    "            \n",
    "    def decode(self, formula):\n",
    "        \"\"\"\n",
    "        Input shape: (seq_len,)\n",
    "        Output Shape: seq_len -> python list\n",
    "        \"\"\"\n",
    "        return \" \".join([self.id_to_wd[idx.item()] for idx in formula])\n",
    "\n",
    "class LatexFormulaDataset(Dataset):\n",
    "    \"\"\"Latex Formula Dataset: Image and Text\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform = None, max_examples = None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with image name and text\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        info(\"Loading Dataset...\")\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        \n",
    "        #info(\"Loaded Dataset\", self.df.info)\n",
    "        \n",
    "        #Slice the dataset if max_examples is not None\n",
    "        if max_examples is not None:\n",
    "            self.df = self.df.iloc[:max_examples, :]\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.df['formula'] = self.df['formula'].apply(lambda x: x.split())\n",
    "        self.df['formula'] = self.df['formula'].apply(lambda x: [START_TOKEN] + x + [END_TOKEN])\n",
    "\n",
    "        self.maxlen = 0\n",
    "        for formula in self.df['formula']:\n",
    "            if len(formula) > self.maxlen:\n",
    "                self.maxlen = len(formula)\n",
    "     \n",
    "        \n",
    "        self.df['formula'] = self.df['formula'].apply(lambda x: x +[PAD_TOKEN]*(max(self.maxlen, MAX_SEQ_LEN) - len(x)))\n",
    "        self.vocab= self.construct_vocab() \n",
    "        # self.df['formula'] = self.df['formula'].apply(convert_to_ids)\n",
    "        info(\"Loaded.\")\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns sample of type image, textformula\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.df.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        formula = self.df.iloc[idx, 1]\n",
    "\n",
    "        # formula = np.array([formula], dtype=str).reshape(-1, 1)\n",
    "        # formula = [self.vocab.get_id(wd[0]) for wd in formula]\n",
    "        \n",
    "        def convert_to_ids(formula):\n",
    "            form2 = [VOCAB.get_id(wd) for wd in formula]\n",
    "            return torch.tensor(form2, dtype=torch.int64)[0:MAX_SEQ_LEN]\n",
    "        \n",
    "        sample = {'image': image, 'formula': convert_to_ids(formula)}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            \n",
    "        return sample \n",
    "    \n",
    "    def construct_vocab(self):\n",
    "        \"\"\"\n",
    "        Constructs vocabulary from the dataset formulas\n",
    "        \"\"\"\n",
    "        #Split on spaces to tokenize\n",
    "        freq_dict = {}\n",
    "        for formula in self.df['formula']:\n",
    "            for wd in formula:\n",
    "                if wd not in freq_dict:\n",
    "                    freq_dict[wd] = 1\n",
    "                else:\n",
    "                    freq_dict[wd] += 1\n",
    "        freq_dict[UNK_TOKEN] = 1\n",
    "        N = len(freq_dict)\n",
    "        wd_to_id = {}\n",
    "        for i, wd in enumerate(freq_dict):\n",
    "            wd_to_id[wd] = i\n",
    "        id_to_wd = {v: k for k, v in wd_to_id.items()}\n",
    "    \n",
    "        #pad the formulas with \n",
    "        return Vocabulary(freq_dict, wd_to_id, id_to_wd)      \n",
    "\n",
    "def get_dataloader(csv_path, image_root, batch_size, transform = None, max_examples = None):\n",
    "    \"\"\"\n",
    "    Returns dataloader,dataset for the dataset\n",
    "    \"\"\"\n",
    "    dataset = LatexFormulaDataset(csv_path, image_root, max_examples=max_examples,transform=transform) #checked\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader, dataset\n",
    " \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    \"\"\"Latex Formula Dataset: Image and Text\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform = None, max_examples = None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with image name and text\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        #info(\"Loaded Dataset\", self.df.info)\n",
    "        \n",
    "        #Slice the dataset if max_examples is not None\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.img_names = os.listdir(self.root_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns sample of type image, textformula\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.img_names[idx]\n",
    "        img_path = os.path.join(self.root_dir, self.img_names[idx])\n",
    "        image = io.imread(img_path)\n",
    "\n",
    "        sample = {'image': image, 'image_id': img_name}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            \n",
    "        return sample "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import vgg16 from torch\n",
    "from torchvision.models import vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    (here M is whatever the batch size is passed)\n",
    "\n",
    "    context_size : size of the context vector [shape: (1,M,context_size)]\n",
    "    n_layers: number of layers [for our purposes, defaults to 1]\n",
    "    hidden_size : size of the hidden state vectors [shape: (n_layers,M,hidden_size)]\n",
    "    embed_size : size of the embedding vectors [shape: (1,M,embed_size)]\n",
    "    vocab_size : size of the vocabulary\n",
    "    max_length : maximum length of the formula\n",
    "    \"\"\"\n",
    "    def __init__(self, context_size, vocab, max_seq_len, n_layers = 1, hidden_size = 512, embed_size = 512):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab.N\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.input_size = context_size + embed_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.embed = nn.Embedding(self.vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, self.vocab_size)\n",
    "    \n",
    "    def forward(self, context, target_tensor = None):\n",
    "        \"\"\"\n",
    "        M: batch_size\n",
    "        context is the context vector from the encoder [shape: (M,context_size)]\n",
    "        target_tensor is the formula in tensor form [shape: (M,max_length)] (in the second dimension, it is sequence of indices of formula tokens)\n",
    "            if target_tensor is not None, then we are in Teacher Forcing mode\n",
    "            else normal jo bhi (last prediction ka embed is concatenated)\n",
    "        \"\"\"\n",
    "        context.to(device)\n",
    "        batch_size = context.shape[0]\n",
    "\n",
    "        #initialize hidden state and cell state\n",
    "        hidden = context\n",
    "        cell = torch.zeros(batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        #initialize the input with embedding of the start token. Expand for batch size.\n",
    "        init_embed = self.embed(torch.tensor([self.vocab.wd_to_id[START_TOKEN]]).to(device).expand(batch_size, -1)).squeeze()\n",
    "        \n",
    "        #initialize the output_history and init_output\n",
    "        outputs = []\n",
    "        output = torch.zeros((batch_size, self.vocab_size)).to(device)\n",
    "        \n",
    "        \n",
    "        for i in range(self.max_seq_len):\n",
    "            #teacher forcing: 50% times\n",
    "            r = torch.rand(1)\n",
    "            if r>0.5 and target_tensor is not None:\n",
    "                if i==0 :\n",
    "                    embedding = init_embed\n",
    "                else: \n",
    "                    embedding = self.embed(target_tensor[:, i-1]).reshape((batch_size, self.embed_size)).to(device)            \n",
    "            else:\n",
    "                if i==0 :\n",
    "                    embedding = init_embed\n",
    "\n",
    "                else:\n",
    "                    #create embedding from previous input\n",
    "                    embedding = self.embed(torch.argmax(output, dim = 1))\n",
    "\n",
    "            lstm_input = torch.cat([context, embedding], dim = 1).to(device)\n",
    "    \n",
    "            hidden, cell = self.lstm(lstm_input, (hidden, cell))\n",
    "            output = self.linear(hidden)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        output_tensor = torch.stack(outputs).permute(1,0,2) #LBV - > BLV\n",
    "\n",
    "        return output_tensor, hidden, cell\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwritingToLatexModel(nn.Module):\n",
    "    def __init__(self, context_size, vocab, max_seq_len, n_layers, hidden_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.encoder = vgg16(weights=\"IMAGENET1K_V1\")\n",
    "        #Freeze weights of the encoder and unfreeze last layer\n",
    "        self.encoder.classifier[-1] = nn.Linear(4096, 512)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.encoder.classifier[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "        self.decoder = DecoderLSTM(context_size, vocab, max_seq_len, n_layers, hidden_size, embed_size)\n",
    "    \n",
    "    def forward(self, image, target_tensor = None):\n",
    "        context = self.encoder(image)\n",
    "        outputs, _, _ = self.decoder(context, target_tensor)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_PATH = \"data/\"\n",
    "SAVE_BASE_PATH = \"checkpoints/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader,model, optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    idx = 0\n",
    "    pb = tqdm(dataloader, desc=\"Batch\")\n",
    "    for data in pb:\n",
    "        idx+=1\n",
    "        input_tensor, target_tensor = data['image'].to(device), data['formula'].to(device)\n",
    "        outputs = model(input_tensor, target_tensor)\n",
    "        train_dataset = dataloader.dataset \n",
    "        if(train_dataset and idx%100==0):\n",
    "            generated_formula = [VOCAB.id_to_wd[token.item()] for token in torch.argmax(outputs, dim=2)[0]]\n",
    "            required_formula = [VOCAB.id_to_wd[token.item()] for token in target_tensor[0]]\n",
    "            print(f\"Generated: {' '.join(generated_formula)}\")\n",
    "            print(f\"Actual: {' '.join(required_formula)}\")\n",
    "\n",
    "        output_logits = outputs.permute(0,2,1)\n",
    "        \n",
    "        loss = criterion(\n",
    "            output_logits,\n",
    "            target_tensor\n",
    "        )\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pb.set_description(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train(train_dataloader, model, n_epochs, optimizer = None, learning_rate=0.001, print_every=1, save_interval=2, save_prefix = 'model'):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    if not optimizer: optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=VOCAB.wd_to_id[PAD_TOKEN]).to(device) #as stated in assignment\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        info(f\"Epoch {epoch}\")\n",
    "        loss = train_epoch(train_dataloader, model, optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "    \n",
    "        if epoch % save_interval == 0:\n",
    "            saveModel(f'{SAVE_BASE_PATH}{save_prefix}_epoch_{epoch}.pt', model.state_dict(), optimizer.state_dict(), loss)    \n",
    "                \n",
    "        info('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Loaded.\n",
      "Loading Dataset...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "#part a\n",
    "batch_size = 32\n",
    "vocab_size = 1000\n",
    "CONTEXT_SIZE = 512\n",
    "HIDDEN_SIZE = 512\n",
    "EMBED_SIZE = 512\n",
    "MAX_EXAMPLES = 1000\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "])\n",
    "\n",
    "transform_hw = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "])\n",
    "\n",
    "train_csv_path = f\"{DATA_BASE_PATH}/SyntheticData/train.csv\"\n",
    "image_root_path = f\"{DATA_BASE_PATH}/SyntheticData/images/\"\n",
    "train_dataloader, train_dataset = get_dataloader(train_csv_path, image_root_path, batch_size, transform, max_examples=None)\n",
    "\n",
    "hw_train_csv_path = f\"{DATA_BASE_PATH}/HandwrittenData/train_hw.csv\"\n",
    "hw_image_root_path = f\"{DATA_BASE_PATH}/HandwrittenData/images/train/\"\n",
    "hw_train_dataloader, hw_train_dataset = get_dataloader(hw_train_csv_path, hw_image_root_path, batch_size, transform_hw, max_examples=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create vocabulary connsisting of both vocabs of synthetic and handwritten datasets\n",
    "def combine_vocabs(v1, v2):\n",
    "    freq_dict = {}\n",
    "    for wd in v1.freq_dict:\n",
    "        freq_dict[wd] = v1.freq_dict[wd]\n",
    "    for wd in v2.freq_dict:\n",
    "        if wd not in freq_dict:\n",
    "            freq_dict[wd] = v2.freq_dict[wd]\n",
    "        else:\n",
    "            freq_dict[wd] += v2.freq_dict[wd]\n",
    "    freq_dict[UNK_TOKEN] = 1\n",
    "    N = len(freq_dict)\n",
    "    wd_to_id = {}\n",
    "    for i, wd in enumerate(freq_dict):\n",
    "        wd_to_id[wd] = i\n",
    "    id_to_wd = {v: k for k, v in wd_to_id.items()}\n",
    "    return Vocabulary(freq_dict, wd_to_id, id_to_wd)\n",
    "\n",
    "VOCAB = combine_vocabs(train_dataset.vocab, hw_train_dataset.vocab)\n",
    "\n",
    "# model = HandwritingToLatexModel(CONTEXT_SIZE, VOCAB, n_layers=1, hidden_size= HIDDEN_SIZE, embed_size=EMBED_SIZE, max_seq_len=MAX_SEQ_LEN).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_checkpoint(checkpoint_path):\n",
    "    model_dicts = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model = HandwritingToLatexModel(CONTEXT_SIZE, VOCAB, n_layers=1, hidden_size= HIDDEN_SIZE, embed_size=EMBED_SIZE, max_seq_len=MAX_SEQ_LEN).to(device)\n",
    "    model.load_state_dict(model_dicts['model_state_dict'])\n",
    "    \n",
    "    optims = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    optims.load_state_dict(model_dicts['optimizer_state_dict'])\n",
    "\n",
    "    return model, optims\n",
    "\n",
    "model_load, optims_load = load_from_checkpoint(f\"{SAVE_BASE_PATH}FT4.0_epoch_8.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, test_dataloader, device):\n",
    "    print('Generating Predictions...')\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    image_ids = []\n",
    "    idx = 0\n",
    "    print(len(test_dataloader))\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        idx+=1\n",
    "        if(idx%20==0):\n",
    "            print(f\"Batch {idx} done\")\n",
    "        input_tensor = batch['image'].to(device)\n",
    "        batch_image_ids = batch['image_id']\n",
    "        outputs = model(input_tensor)\n",
    "        outputs_final = torch.argmax(outputs, dim=2)\n",
    "\n",
    "        for j in range(len(outputs)):\n",
    "            gen_formula = VOCAB.decode(outputs_final[j]).split()\n",
    "            # gen_formula = [train_dataset.vocab.id_to_wd[token.item()] for token in outputs_final[j]]\n",
    "            gen_trim = []\n",
    "            for tken in gen_formula:\n",
    "                if tken == END_TOKEN:\n",
    "                    break\n",
    "                gen_trim.append(tken)\n",
    "            gen_formula = gen_trim[1:]\n",
    "            gen_formula = ' '.join(gen_formula)\n",
    "\n",
    "            predictions.append(gen_formula)\n",
    "            image_ids.append(batch_image_ids[j])\n",
    "    print('Predictions Generated!')\n",
    "    return image_ids, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_root_path = os.path.join(\"data\", \"HandwrittenData/Images/test/\")\n",
    "test_dataloader = DataLoader(TestDataset(test_image_root_path, transform=transform_hw), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Predictions...\n",
      "133\n",
      "Batch 20 done\n",
      "Batch 40 done\n",
      "Batch 60 done\n",
      "Batch 80 done\n",
      "Batch 100 done\n",
      "Batch 120 done\n",
      "Predictions Generated!\n"
     ]
    }
   ],
   "source": [
    "image_ids, predictions = get_predictions(model_load, test_dataloader, device)\n",
    "\n",
    "#create file for output (image_id, predicted_formula) dataframe\n",
    "output_df = pd.DataFrame({'image':image_ids, 'formula':predictions})\n",
    "\n",
    "#save output dataframe as csv: handwritten images\n",
    "output_df.to_csv('FT4.0_8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
